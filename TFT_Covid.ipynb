{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e01e89f-a3ba-4cb1-b386-662a823e7273",
   "metadata": {},
   "source": [
    "### Temporal Fusion Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "8e98e3d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T01:24:31.771768Z",
     "iopub.status.busy": "2022-10-02T01:24:31.771206Z",
     "iopub.status.idle": "2022-10-02T01:24:31.795835Z",
     "shell.execute_reply": "2022-10-02T01:24:31.794906Z",
     "shell.execute_reply.started": "2022-10-02T01:24:31.771712Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from darts import TimeSeries\n",
    "from darts.metrics import rmse\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "torch.manual_seed(3407)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from darts.models import TFTModel "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcb867e2-a37b-46b1-9c78-5d7918f90768",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-01T13:51:31.435839Z",
     "iopub.status.busy": "2022-10-01T13:51:31.435656Z",
     "iopub.status.idle": "2022-10-01T13:51:31.439804Z",
     "shell.execute_reply": "2022-10-01T13:51:31.438701Z",
     "shell.execute_reply.started": "2022-10-01T13:51:31.435818Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mm_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d77b3b7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-01T14:18:10.979293Z",
     "iopub.status.busy": "2022-10-01T14:18:10.979037Z",
     "iopub.status.idle": "2022-10-01T14:18:10.989863Z",
     "shell.execute_reply": "2022-10-01T14:18:10.988728Z",
     "shell.execute_reply.started": "2022-10-01T14:18:10.979266Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "canada = pd.read_csv('canada.csv')\n",
    "canada = canada.values[:, 0:3].astype(float)\n",
    "\n",
    "TRAIN_IDX = int(canada.shape[0]*.5)\n",
    "VAL_IDX = TRAIN_IDX + int(canada.shape[0]*.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb4a4153-511c-4dfd-b4e7-14ebed1e4311",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-01T14:18:16.945855Z",
     "iopub.status.busy": "2022-10-01T14:18:16.945222Z",
     "iopub.status.idle": "2022-10-01T14:18:16.955668Z",
     "shell.execute_reply": "2022-10-01T14:18:16.954520Z",
     "shell.execute_reply.started": "2022-10-01T14:18:16.945806Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "canada[:TRAIN_IDX] = mm_scaler.fit_transform(canada[:TRAIN_IDX])\n",
    "canada[TRAIN_IDX:VAL_IDX] = mm_scaler.transform(canada[TRAIN_IDX:VAL_IDX])\n",
    "canada_train_val = canada[:VAL_IDX]\n",
    "\n",
    "time_series = TimeSeries.from_values(canada_train_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70e1e818",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-01T14:18:16.958900Z",
     "iopub.status.busy": "2022-10-01T14:18:16.958569Z",
     "iopub.status.idle": "2022-10-01T15:23:33.981339Z",
     "shell.execute_reply": "2022-10-01T15:23:33.980213Z",
     "shell.execute_reply.started": "2022-10-01T14:18:16.958862Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c51f27a25b1440980f1b08273fea755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/216 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-01 16:23:33,949] INFO | darts.models.forecasting.forecasting_model | Chosen parameters: {'hidden_continuous_size': 8, 'lstm_layers': 2, 'dropout': 0.1, 'batch_size': 4, 'input_chunk_length': 32, 'output_chunk_length': 2, 'n_epochs': 24, 'num_attention_heads': 2, 'add_relative_index': True}\n",
      "[2022-10-01 16:23:33,949] INFO | darts.models.forecasting.forecasting_model | Chosen parameters: {'hidden_continuous_size': 8, 'lstm_layers': 2, 'dropout': 0.1, 'batch_size': 4, 'input_chunk_length': 32, 'output_chunk_length': 2, 'n_epochs': 24, 'num_attention_heads': 2, 'add_relative_index': True}\n",
      "2022-10-01 16:23:33 darts.models.forecasting.forecasting_model INFO: Chosen parameters: {'hidden_continuous_size': 8, 'lstm_layers': 2, 'dropout': 0.1, 'batch_size': 4, 'input_chunk_length': 32, 'output_chunk_length': 2, 'n_epochs': 24, 'num_attention_heads': 2, 'add_relative_index': True}\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_STEP = 2  # predict |OUTPUT_STEP| steps as output\n",
    "\n",
    "parameters = {\n",
    "    \"hidden_continuous_size\": [2, 4, 8],\n",
    "    \"lstm_layers\": [2, 4],\n",
    "    \"dropout\": [0.1, 0.3],   \n",
    "    \"batch_size\": [2,4],\n",
    "    \"input_chunk_length\": [16,32,64],\n",
    "    \"output_chunk_length\": [OUTPUT_STEP],\n",
    "    \"n_epochs\": [24],\n",
    "    \"num_attention_heads\": [2, 4, 8],   # divisible by number of features\n",
    "    \"add_relative_index\": [True]\n",
    "}\n",
    "\n",
    "best_model, _, _ = TFTModel.gridsearch(\n",
    "    parameters = parameters,\n",
    "    series = time_series[:TRAIN_IDX],\n",
    "    val_series = time_series[TRAIN_IDX:VAL_IDX],\n",
    "    metric = rmse,\n",
    "    n_jobs=16,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "992b43ce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-01T15:23:33.985827Z",
     "iopub.status.busy": "2022-10-01T15:23:33.985528Z",
     "iopub.status.idle": "2022-10-01T15:24:17.308856Z",
     "shell.execute_reply": "2022-10-01T15:24:17.307429Z",
     "shell.execute_reply.started": "2022-10-01T15:23:33.985790Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-01 16:23:33,995] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 244 samples.\n",
      "[2022-10-01 16:23:33,995] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 244 samples.\n",
      "2022-10-01 16:23:33 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 244 samples.\n",
      "[2022-10-01 16:23:34,046] INFO | darts.models.forecasting.torch_forecasting_model | Time series values are 64-bits; casting model to float64.\n",
      "[2022-10-01 16:23:34,046] INFO | darts.models.forecasting.torch_forecasting_model | Time series values are 64-bits; casting model to float64.\n",
      "2022-10-01 16:23:34 darts.models.forecasting.torch_forecasting_model INFO: Time series values are 64-bits; casting model to float64.\n",
      "2022-10-01 16:23:34 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-01 16:23:34 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-01 16:23:34 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "2022-10-01 16:23:34 pytorch_lightning.callbacks.model_summary INFO: \n",
      "   | Name                              | Type                             | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
      "1  | encoder_vsn                       | _VariableSelectionNetwork        | 2.4 K \n",
      "2  | decoder_vsn                       | _VariableSelectionNetwork        | 528   \n",
      "3  | static_context_grn                | _GatedResidualNetwork            | 1.1 K \n",
      "4  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 1.1 K \n",
      "5  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 1.1 K \n",
      "6  | static_context_enrichment         | _GatedResidualNetwork            | 1.1 K \n",
      "7  | lstm_encoder                      | LSTM                             | 4.4 K \n",
      "8  | lstm_decoder                      | LSTM                             | 4.4 K \n",
      "9  | post_lstm_gan                     | _GateAddNorm                     | 576   \n",
      "10 | static_enrichment_grn             | _GatedResidualNetwork            | 1.4 K \n",
      "11 | multihead_attn                    | _InterpretableMultiHeadAttention | 808   \n",
      "12 | post_attn_gan                     | _GateAddNorm                     | 576   \n",
      "13 | positionwise_feedforward_grn      | _GatedResidualNetwork            | 1.1 K \n",
      "14 | pre_output_gan                    | _GateAddNorm                     | 576   \n",
      "15 | output_layer                      | Linear                           | 867   \n",
      "----------------------------------------------------------------------------------------\n",
      "22.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "22.0 K    Total params\n",
      "0.176     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4939874e23df48818d92c08b9c406679",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<darts.models.forecasting.tft_model.TFTModel at 0x7fe84eff0d90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.fit(time_series[:TRAIN_IDX]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e03b28e-77f6-4117-b295-d47699bd49d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-01T15:24:17.312594Z",
     "iopub.status.busy": "2022-10-01T15:24:17.312102Z",
     "iopub.status.idle": "2022-10-01T15:24:17.523957Z",
     "shell.execute_reply": "2022-10-01T15:24:17.523062Z",
     "shell.execute_reply.started": "2022-10-01T15:24:17.312542Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model.save_model('tft_train_val.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6e42e4c3-ec78-40c0-bc52-5f16452f5dcc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T10:15:07.572261Z",
     "iopub.status.busy": "2022-10-02T10:15:07.570398Z",
     "iopub.status.idle": "2022-10-02T10:15:07.688132Z",
     "shell.execute_reply": "2022-10-02T10:15:07.687165Z",
     "shell.execute_reply.started": "2022-10-02T10:15:07.572160Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_model = TFTModel.load_model('tft_train_val.pth.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "b4f8f2ae-4d11-48bc-a968-aaef41fde05d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T10:15:07.690269Z",
     "iopub.status.busy": "2022-10-02T10:15:07.690045Z",
     "iopub.status.idle": "2022-10-02T10:15:07.917680Z",
     "shell.execute_reply": "2022-10-02T10:15:07.916968Z",
     "shell.execute_reply.started": "2022-10-02T10:15:07.690240Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-02 11:15:07 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:07 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:07 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5825710756466c99b44aeecefae9a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.996590845529638\n"
     ]
    }
   ],
   "source": [
    "predict = best_model.predict(VAL_IDX-TRAIN_IDX)\n",
    "print(rmse(predict, time_series[TRAIN_IDX:VAL_IDX]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "6e475d11-a18d-4a15-8b60-bda48d5627f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T10:15:07.920391Z",
     "iopub.status.busy": "2022-10-02T10:15:07.920155Z",
     "iopub.status.idle": "2022-10-02T10:15:07.924966Z",
     "shell.execute_reply": "2022-10-02T10:15:07.924005Z",
     "shell.execute_reply.started": "2022-10-02T10:15:07.920364Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inversed_predict=mm_scaler.inverse_transform(predict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "7faf8be9-be97-4a7a-bbfa-7851970758d4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T10:15:07.927625Z",
     "iopub.status.busy": "2022-10-02T10:15:07.927190Z",
     "iopub.status.idle": "2022-10-02T10:15:07.932289Z",
     "shell.execute_reply": "2022-10-02T10:15:07.931220Z",
     "shell.execute_reply.started": "2022-10-02T10:15:07.927592Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "true_value=mm_scaler.inverse_transform(time_series[TRAIN_IDX:VAL_IDX].values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "0f66db1a-b6b2-4b04-b84f-05db964bda7e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T10:15:07.934663Z",
     "iopub.status.busy": "2022-10-02T10:15:07.934164Z",
     "iopub.status.idle": "2022-10-02T10:15:08.105754Z",
     "shell.execute_reply": "2022-10-02T10:15:08.104972Z",
     "shell.execute_reply.started": "2022-10-02T10:15:07.934615Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = range(len(inversed_predict))\n",
    "\n",
    "plt.plot(x, inversed_predict[:,0], label ='predict infected')\n",
    "plt.plot(x, true_value[:,0], '-.', label ='actual infected')\n",
    "plt.plot(x, inversed_predict[:,1], label ='predict recovered')\n",
    "plt.plot(x, true_value[:,1], '-.', label ='actual recovered')\n",
    "\n",
    "plt.xlabel(\"date\")\n",
    "plt.ylabel(\"number of infected\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d95da207-0841-4a4f-9059-21b59d998a63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T10:15:08.107459Z",
     "iopub.status.busy": "2022-10-02T10:15:08.107174Z",
     "iopub.status.idle": "2022-10-02T10:15:08.114099Z",
     "shell.execute_reply": "2022-10-02T10:15:08.113289Z",
     "shell.execute_reply.started": "2022-10-02T10:15:08.107429Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "178626.20636945663"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(inversed_predict[:,0], true_value[:,0], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "69df9bb3-2473-493c-a482-15575d0c89d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T10:15:08.115431Z",
     "iopub.status.busy": "2022-10-02T10:15:08.115078Z",
     "iopub.status.idle": "2022-10-02T10:15:08.121113Z",
     "shell.execute_reply": "2022-10-02T10:15:08.120058Z",
     "shell.execute_reply.started": "2022-10-02T10:15:08.115395Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "canada_test = canada[VAL_IDX-best_model.input_chunk_length:]  # start to predict from here\n",
    "test_time_transforms = TimeSeries.from_values(mm_scaler.transform(canada_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "caede684-c4c7-4ef4-9027-4c779eaaa823",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T10:15:08.123259Z",
     "iopub.status.busy": "2022-10-02T10:15:08.122832Z",
     "iopub.status.idle": "2022-10-02T10:19:47.908633Z",
     "shell.execute_reply": "2022-10-02T10:19:47.907861Z",
     "shell.execute_reply.started": "2022-10-02T10:15:08.123213Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e6190bd784244529ea6fdd3bf30cc48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:08,181] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 31 samples.\n",
      "[2022-10-02 11:15:08,181] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 31 samples.\n",
      "2022-10-02 11:15:08 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 31 samples.\n",
      "2022-10-02 11:15:08 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:08 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:08 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "2022-10-02 11:15:08 pytorch_lightning.callbacks.model_summary INFO: \n",
      "   | Name                              | Type                             | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
      "1  | encoder_vsn                       | _VariableSelectionNetwork        | 2.4 K \n",
      "2  | decoder_vsn                       | _VariableSelectionNetwork        | 528   \n",
      "3  | static_context_grn                | _GatedResidualNetwork            | 1.1 K \n",
      "4  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 1.1 K \n",
      "5  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 1.1 K \n",
      "6  | static_context_enrichment         | _GatedResidualNetwork            | 1.1 K \n",
      "7  | lstm_encoder                      | LSTM                             | 4.4 K \n",
      "8  | lstm_decoder                      | LSTM                             | 4.4 K \n",
      "9  | post_lstm_gan                     | _GateAddNorm                     | 576   \n",
      "10 | static_enrichment_grn             | _GatedResidualNetwork            | 1.4 K \n",
      "11 | multihead_attn                    | _InterpretableMultiHeadAttention | 808   \n",
      "12 | post_attn_gan                     | _GateAddNorm                     | 576   \n",
      "13 | positionwise_feedforward_grn      | _GatedResidualNetwork            | 1.1 K \n",
      "14 | pre_output_gan                    | _GateAddNorm                     | 576   \n",
      "15 | output_layer                      | Linear                           | 867   \n",
      "----------------------------------------------------------------------------------------\n",
      "22.0 K    Trainable params\n",
      "0         Non-trainable params\n",
      "22.0 K    Total params\n",
      "0.176     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e540131063d14bcb8433fd6d979d0ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddbfbc30f7904c5ab47a9d04a17deec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 8it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:08,557] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 32 samples.\n",
      "[2022-10-02 11:15:08,557] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 32 samples.\n",
      "2022-10-02 11:15:08 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 32 samples.\n",
      "2022-10-02 11:15:08 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:08 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:08 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:08,564] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:08,564] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:08 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056f492b65d84b0086ebd5aaa1fa43b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0e624975f90420c9197148d56b1782e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 8it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:08,924] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 33 samples.\n",
      "[2022-10-02 11:15:08,924] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 33 samples.\n",
      "2022-10-02 11:15:08 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 33 samples.\n",
      "2022-10-02 11:15:08 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:08 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:08 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:08,931] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:08,931] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:08 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c269f43217d4c9bb13eb4dbb59aa3c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a034291c27c423c87ebaf97d77f05c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 9it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:09,314] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 34 samples.\n",
      "[2022-10-02 11:15:09,314] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 34 samples.\n",
      "2022-10-02 11:15:09 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 34 samples.\n",
      "2022-10-02 11:15:09 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:09 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:09 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:09,322] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:09,322] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:09 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd20e17989b2464f99b931c7ef0d41a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9799c8be294adeac26e49a81962d1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 9it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:09,695] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 35 samples.\n",
      "[2022-10-02 11:15:09,695] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 35 samples.\n",
      "2022-10-02 11:15:09 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 35 samples.\n",
      "2022-10-02 11:15:09 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:09 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:09 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:09,703] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:09,703] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:09 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4db6027bc52841fe8d3f8c8a5d085687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a172f3c05584efebd73c09153b2951b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 9it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:10,095] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 36 samples.\n",
      "[2022-10-02 11:15:10,095] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 36 samples.\n",
      "2022-10-02 11:15:10 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 36 samples.\n",
      "2022-10-02 11:15:10 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:10 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:10 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:10,102] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:10,102] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:10 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64e4bb135d834df2a95f7dfac36a09d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69ab157fd9b04a1788622e9d2133f376",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 9it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:10,483] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 37 samples.\n",
      "[2022-10-02 11:15:10,483] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 37 samples.\n",
      "2022-10-02 11:15:10 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 37 samples.\n",
      "2022-10-02 11:15:10 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:10 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:10 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:10,491] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:10,491] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:10 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9236705754c74073b8a43f608d943878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626f1a42f6f54266aa46033fabb3f837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 10it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:10,880] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 38 samples.\n",
      "[2022-10-02 11:15:10,880] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 38 samples.\n",
      "2022-10-02 11:15:10 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 38 samples.\n",
      "2022-10-02 11:15:10 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:10 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:10 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:10,887] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:10,887] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:10 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "991d78f2301e49c383339dc0458ed89b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7176a1e13b484db3923eea599c911634",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 10it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:11,305] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 39 samples.\n",
      "[2022-10-02 11:15:11,305] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 39 samples.\n",
      "2022-10-02 11:15:11 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 39 samples.\n",
      "2022-10-02 11:15:11 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:11 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:11 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:11,313] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:11,313] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:11 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f887879e4ffb400d9ec5a80b6de2b436",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f31c76534784deb9d34e0f807e30401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 10it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:11,743] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 40 samples.\n",
      "[2022-10-02 11:15:11,743] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 40 samples.\n",
      "2022-10-02 11:15:11 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 40 samples.\n",
      "2022-10-02 11:15:11 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:11 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:11 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:11,753] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:11,753] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:11 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029fd3cc5c354abf9dc2d6496c3fcaa5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646ba0e142dd4265b6b0f9213f29c278",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 10it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:12,177] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 41 samples.\n",
      "[2022-10-02 11:15:12,177] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 41 samples.\n",
      "2022-10-02 11:15:12 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 41 samples.\n",
      "2022-10-02 11:15:12 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:12 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:12 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:12,184] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:12,184] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:12 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0f0707aa814ac6a4bab9f9147bbdd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab825b11c4b64a4fbeba32550dc13801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 11it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:12,616] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 42 samples.\n",
      "[2022-10-02 11:15:12,616] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 42 samples.\n",
      "2022-10-02 11:15:12 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 42 samples.\n",
      "2022-10-02 11:15:12 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:12 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:12 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:12,624] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:12,624] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:12 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ab8f1a853b42f59d230f30dfb276ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99ea85b6bc74585a7e9cad0b234ca63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 11it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:13,065] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 43 samples.\n",
      "[2022-10-02 11:15:13,065] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 43 samples.\n",
      "2022-10-02 11:15:13 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 43 samples.\n",
      "2022-10-02 11:15:13 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:13 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:13 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:13,073] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:13,073] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:13 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4e95fdcb80439ab2ab3a34e3f72f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1770a3f6718a44cfaf0681d823bc551b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 11it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:13,519] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 44 samples.\n",
      "[2022-10-02 11:15:13,519] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 44 samples.\n",
      "2022-10-02 11:15:13 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 44 samples.\n",
      "2022-10-02 11:15:13 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:13 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:13 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:13,526] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:13,526] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:13 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25b30a6f8e974e9ba7f20f12a6bf0534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5909011a075343bd92ab04e931cb4516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 11it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:13,942] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 45 samples.\n",
      "[2022-10-02 11:15:13,942] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 45 samples.\n",
      "2022-10-02 11:15:13 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 45 samples.\n",
      "2022-10-02 11:15:13 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:13 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:13 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:13,949] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:13,949] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:13 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84e17b45fcf84b77b76f390f26c4d189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3361545c8344cf88d6c2774b882bd41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 12it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:14,392] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 46 samples.\n",
      "[2022-10-02 11:15:14,392] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 46 samples.\n",
      "2022-10-02 11:15:14 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 46 samples.\n",
      "2022-10-02 11:15:14 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:14 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:14 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:14,399] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:14,399] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:14 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2286efd8d8a4d0caf027ced5622a40c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f26c322f2354d1c8d1fc12b5f9b8cd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 12it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:14,856] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 47 samples.\n",
      "[2022-10-02 11:15:14,856] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 47 samples.\n",
      "2022-10-02 11:15:14 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 47 samples.\n",
      "2022-10-02 11:15:14 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:14 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:14 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:14,862] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:14,862] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:14 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "609949613bdc4801b6b58d798a3330cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3fd140d55248e0ae3a97a167eb6c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 12it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:15,318] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 48 samples.\n",
      "[2022-10-02 11:15:15,318] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 48 samples.\n",
      "2022-10-02 11:15:15 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 48 samples.\n",
      "2022-10-02 11:15:15 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:15 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:15 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:15,325] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:15,325] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:15 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97bf0c58e0e4c7697af2687ff75e8bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116e2302b2e743b7b407f78c2f970a37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 12it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:15,813] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 49 samples.\n",
      "[2022-10-02 11:15:15,813] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 49 samples.\n",
      "2022-10-02 11:15:15 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 49 samples.\n",
      "2022-10-02 11:15:15 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:15 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:15 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:15,820] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:15,820] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:15 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f023e22801944397a6bea8fa80421231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9db274bfc1045a7b5b8ccf092a43194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 13it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:16,321] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 50 samples.\n",
      "[2022-10-02 11:15:16,321] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 50 samples.\n",
      "2022-10-02 11:15:16 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 50 samples.\n",
      "2022-10-02 11:15:16 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:16 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:16 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:16,327] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:16,327] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:16 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c23c21961beb41d5985aa91bc557844d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1d2ca7c5aa46fdad762df1156b8ff3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 13it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:16,804] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 51 samples.\n",
      "[2022-10-02 11:15:16,804] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 51 samples.\n",
      "2022-10-02 11:15:16 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 51 samples.\n",
      "2022-10-02 11:15:16 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:16 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:16 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:16,812] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:16,812] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:16 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26b1910a261b4c5a8771d3d6613b21fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0029d212414c46119344fc3f614b88ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 13it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:17,340] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 52 samples.\n",
      "[2022-10-02 11:15:17,340] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 52 samples.\n",
      "2022-10-02 11:15:17 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 52 samples.\n",
      "2022-10-02 11:15:17 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:17 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:17 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:17,347] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:17,347] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:17 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0776a5ee3ea04fef8ff2200c165b118e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14242b17ddf5466fb067614d4c4af4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 13it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:17,873] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 53 samples.\n",
      "[2022-10-02 11:15:17,873] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 53 samples.\n",
      "2022-10-02 11:15:17 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 53 samples.\n",
      "2022-10-02 11:15:17 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:17 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:17 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:17,880] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:17,880] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:17 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c01849c4cfb48b79c509f9d6a07b888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d888f1ee9c394f87bca051c6597924d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 14it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:18,392] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 54 samples.\n",
      "[2022-10-02 11:15:18,392] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 54 samples.\n",
      "2022-10-02 11:15:18 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 54 samples.\n",
      "2022-10-02 11:15:18 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:18 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:18 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:18,399] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:18,399] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:18 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a948cb20ce04195a6b252bcd3242607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0790e9867dd47c6881362049928fd44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 14it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:18,935] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 55 samples.\n",
      "[2022-10-02 11:15:18,935] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 55 samples.\n",
      "2022-10-02 11:15:18 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 55 samples.\n",
      "2022-10-02 11:15:18 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:18 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:18 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:18,942] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:18,942] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:18 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11ca10fa41ef4966a1784fde6bddc8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2e8c53038a548aa9f573f86340bb6da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 14it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:19,442] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 56 samples.\n",
      "[2022-10-02 11:15:19,442] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 56 samples.\n",
      "2022-10-02 11:15:19 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 56 samples.\n",
      "2022-10-02 11:15:19 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:19 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:19 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:19,449] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:19,449] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:19 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cceed1d82e54f69acb09c08d3dae043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44680b8d7e19474095733a10c83918de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 14it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:19,956] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 57 samples.\n",
      "[2022-10-02 11:15:19,956] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 57 samples.\n",
      "2022-10-02 11:15:19 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 57 samples.\n",
      "2022-10-02 11:15:19 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:19 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:19 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:19,967] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:19,967] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:19 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65ec7a7c64c8429994f3df4b43349efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1d6c59c2c44d47ac43e591c9caa2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 15it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:20,551] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 58 samples.\n",
      "[2022-10-02 11:15:20,551] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 58 samples.\n",
      "2022-10-02 11:15:20 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 58 samples.\n",
      "2022-10-02 11:15:20 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:20 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:20 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:20,558] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:20,558] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:20 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e142dc1a1914bf4a5af91e803e7b3d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5e8c097931f426187e181c5582cfca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 15it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:21,134] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 59 samples.\n",
      "[2022-10-02 11:15:21,134] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 59 samples.\n",
      "2022-10-02 11:15:21 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 59 samples.\n",
      "2022-10-02 11:15:21 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:21 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:21 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:21,141] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:21,141] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:21 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845be3a5166141d891eb3971cc1c8ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "843bd2f1dd0b49b18165ced8567ded7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 15it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:21,678] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 60 samples.\n",
      "[2022-10-02 11:15:21,678] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 60 samples.\n",
      "2022-10-02 11:15:21 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 60 samples.\n",
      "2022-10-02 11:15:21 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:21 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:21 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:21,685] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:21,685] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:21 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00c48fc9f402407594264762acd56b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7e5d5bc387d44ee945e7ab7bd154df4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 15it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:22,244] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 61 samples.\n",
      "[2022-10-02 11:15:22,244] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 61 samples.\n",
      "2022-10-02 11:15:22 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 61 samples.\n",
      "2022-10-02 11:15:22 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:22 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:22 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:22,250] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:22,250] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:22 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1680862a1da749a9a1fc9568b19e9387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d8cc9e61d04972bb4ae186bf4db454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 16it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:22,854] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 62 samples.\n",
      "[2022-10-02 11:15:22,854] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 62 samples.\n",
      "2022-10-02 11:15:22 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 62 samples.\n",
      "2022-10-02 11:15:22 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:22 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:22 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:22,861] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:22,861] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:22 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b430a22bbfbd4e08b3c723b6273270b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14adafb327144cbea8a4e9e4838ef5bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 16it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:23,439] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 63 samples.\n",
      "[2022-10-02 11:15:23,439] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 63 samples.\n",
      "2022-10-02 11:15:23 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 63 samples.\n",
      "2022-10-02 11:15:23 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:23 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:23 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:23,447] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:23,447] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:23 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312a87bcdd174f3c8dd7b8a848cebe4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8374219efe4c10b763bd414d661f84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 16it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:24,005] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 64 samples.\n",
      "[2022-10-02 11:15:24,005] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 64 samples.\n",
      "2022-10-02 11:15:24 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 64 samples.\n",
      "2022-10-02 11:15:24 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:24 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:24 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:24,013] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:24,013] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:24 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51bde54043c241699f4e4df9ec4d2d54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59f7a7c5e8ae4281811308acc43872fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 16it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:24,566] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 65 samples.\n",
      "[2022-10-02 11:15:24,566] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 65 samples.\n",
      "2022-10-02 11:15:24 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 65 samples.\n",
      "2022-10-02 11:15:24 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:24 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:24 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:24,573] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:24,573] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:24 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b597bb800b9f4c7cb6a1775af4116a2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05967eab3bd34e4ca1fc35dfe4eb6dc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 17it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:25,216] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 66 samples.\n",
      "[2022-10-02 11:15:25,216] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 66 samples.\n",
      "2022-10-02 11:15:25 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 66 samples.\n",
      "2022-10-02 11:15:25 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:25 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:25 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:25,223] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:25,223] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:25 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a547851cc33413d8d71cfa687d0a4bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25cc0f96d424462bf1f68138fc5aebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 17it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:25,820] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 67 samples.\n",
      "[2022-10-02 11:15:25,820] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 67 samples.\n",
      "2022-10-02 11:15:25 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 67 samples.\n",
      "2022-10-02 11:15:25 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:25 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:25 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:25,827] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:25,827] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:25 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efe1c297766c4ef991e377c3b9a19f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f30785dbcc2492daabfda8022ee4478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 17it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:26,451] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 68 samples.\n",
      "[2022-10-02 11:15:26,451] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 68 samples.\n",
      "2022-10-02 11:15:26 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 68 samples.\n",
      "2022-10-02 11:15:26 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:26 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:26 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:26,458] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:26,458] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:26 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16621d19749c4cc996f7bc382ec98f12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a3fb0504ee414a841e695d2450886b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 17it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:27,091] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 69 samples.\n",
      "[2022-10-02 11:15:27,091] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 69 samples.\n",
      "2022-10-02 11:15:27 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 69 samples.\n",
      "2022-10-02 11:15:27 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:27 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:27 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:27,099] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:27,099] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:27 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2e9483af794f298bbb4ec4d6ae4457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433e19cf0feb478ab2d7efa4e42f9c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 18it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:27,719] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 70 samples.\n",
      "[2022-10-02 11:15:27,719] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 70 samples.\n",
      "2022-10-02 11:15:27 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 70 samples.\n",
      "2022-10-02 11:15:27 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:27 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:27 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:27,726] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:27,726] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:27 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a9682f037c4bb5979f56239d8c5dba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b2eb297a4a4127a4ee34ec4e01915e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 18it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:28,329] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 71 samples.\n",
      "[2022-10-02 11:15:28,329] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 71 samples.\n",
      "2022-10-02 11:15:28 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 71 samples.\n",
      "2022-10-02 11:15:28 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:28 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:28 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:28,336] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:28,336] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:28 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c6e99b5d3af47bdad376e1ff97b178d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0db54a3dcd644e6e9719a3de32d3d9fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 18it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:28,981] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 72 samples.\n",
      "[2022-10-02 11:15:28,981] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 72 samples.\n",
      "2022-10-02 11:15:28 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 72 samples.\n",
      "2022-10-02 11:15:28 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:28 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:28 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:28,987] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:28,987] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:28 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beffd9c43f834713b7adcdad4a50036d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bade87a2669471b81ec1f9788b5c02d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 18it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:29,597] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 73 samples.\n",
      "[2022-10-02 11:15:29,597] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 73 samples.\n",
      "2022-10-02 11:15:29 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 73 samples.\n",
      "2022-10-02 11:15:29 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:29 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:29 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:29,604] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:29,604] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:29 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f430ce1c78ee4acc9ba982aa6eb97fa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3dad53b0b14fcd82c6caeac1a3f546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 19it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:30,238] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 74 samples.\n",
      "[2022-10-02 11:15:30,238] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 74 samples.\n",
      "2022-10-02 11:15:30 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 74 samples.\n",
      "2022-10-02 11:15:30 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:30 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:30 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:30,245] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:30,245] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:30 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b93be5f1eef4732b8ba215a3ca7fb44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2505691a86e84f33bad34b268a1d57e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 19it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:30,876] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 75 samples.\n",
      "[2022-10-02 11:15:30,876] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 75 samples.\n",
      "2022-10-02 11:15:30 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 75 samples.\n",
      "2022-10-02 11:15:30 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:30 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:30 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:30,883] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:30,883] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:30 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a6ff49d9c2640538fcfcda323e63041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d865c4fcc3604e71b833bf322b9096aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 19it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:31,536] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 76 samples.\n",
      "[2022-10-02 11:15:31,536] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 76 samples.\n",
      "2022-10-02 11:15:31 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 76 samples.\n",
      "2022-10-02 11:15:31 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:31 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:31 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:31,543] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:31,543] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:31 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771d8f3b4a0a404e869c7b33234d8117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69be56f2ba29482eb83338d9ed86ca0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 19it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:32,220] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 77 samples.\n",
      "[2022-10-02 11:15:32,220] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 77 samples.\n",
      "2022-10-02 11:15:32 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 77 samples.\n",
      "2022-10-02 11:15:32 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:32 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:32 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:32,227] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:32,227] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:32 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c7c692313234d6db68eb02cd71fecc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c7a57ba597143c08bc0e7d967a40e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 20it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:32,906] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 78 samples.\n",
      "[2022-10-02 11:15:32,906] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 78 samples.\n",
      "2022-10-02 11:15:32 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 78 samples.\n",
      "2022-10-02 11:15:32 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:32 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:32 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:32,912] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:32,912] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:32 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e712d01ba74741f987e0ed33567bcf02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9aafe8584146639d188adcdb5a7c5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 20it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:33,574] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 79 samples.\n",
      "[2022-10-02 11:15:33,574] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 79 samples.\n",
      "2022-10-02 11:15:33 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 79 samples.\n",
      "2022-10-02 11:15:33 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:33 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:33 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:33,581] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:33,581] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:33 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3dd3f6e43b44a1eb4d228ec301eeaeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6448898868c748f4a86265c1512a300d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 20it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:34,245] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 80 samples.\n",
      "[2022-10-02 11:15:34,245] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 80 samples.\n",
      "2022-10-02 11:15:34 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 80 samples.\n",
      "2022-10-02 11:15:34 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:34 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:34 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:34,254] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:34,254] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:34 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64a994397f04aaca3965b5d7f682a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4c27cd4754f4a4b976a92e82535ec4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 20it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:34,963] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 81 samples.\n",
      "[2022-10-02 11:15:34,963] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 81 samples.\n",
      "2022-10-02 11:15:34 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 81 samples.\n",
      "2022-10-02 11:15:34 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:34 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:34 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:34,970] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:34,970] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:34 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "638af1f3344d42dfbd2d76fdba31f040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c19f8dfd0bd4cf6b01d6111673a2502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 21it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:35,705] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 82 samples.\n",
      "[2022-10-02 11:15:35,705] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 82 samples.\n",
      "2022-10-02 11:15:35 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 82 samples.\n",
      "2022-10-02 11:15:35 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:35 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:35 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:35,713] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:35,713] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:35 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add201fb6e4b456aa96579e3d1c7cfc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c46d1b7927684eeb88f225b9f7275def",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 21it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:36,436] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 83 samples.\n",
      "[2022-10-02 11:15:36,436] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 83 samples.\n",
      "2022-10-02 11:15:36 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 83 samples.\n",
      "2022-10-02 11:15:36 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:36 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:36 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:36,443] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:36,443] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:36 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a65788a1d1264bc3b7fa5716ceabe4dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "352197ac82364e40a951aca6b08256b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 21it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:37,272] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 84 samples.\n",
      "[2022-10-02 11:15:37,272] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 84 samples.\n",
      "2022-10-02 11:15:37 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 84 samples.\n",
      "2022-10-02 11:15:37 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:37 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:37 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:37,282] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:37,282] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:37 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8e484a6adf4b7db2df0d0e9b746453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d93fd1ee2374937a6d591c436f7a83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 21it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:38,025] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 85 samples.\n",
      "[2022-10-02 11:15:38,025] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 85 samples.\n",
      "2022-10-02 11:15:38 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 85 samples.\n",
      "2022-10-02 11:15:38 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:38 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:38 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:38,032] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:38,032] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:38 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "475843228dbf4af082cbe67ea11ccc75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eadb544ef6c04a1c9220c8df194fced7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 22it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:38,787] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 86 samples.\n",
      "[2022-10-02 11:15:38,787] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 86 samples.\n",
      "2022-10-02 11:15:38 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 86 samples.\n",
      "2022-10-02 11:15:38 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:38 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:38 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:38,795] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:38,795] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:38 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab39ad4002a48dc9f1961d9f3aeb22f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956c7a2dd93b4d12a02c70d3c1cbe2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 22it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:39,531] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 87 samples.\n",
      "[2022-10-02 11:15:39,531] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 87 samples.\n",
      "2022-10-02 11:15:39 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 87 samples.\n",
      "2022-10-02 11:15:39 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:39 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:39 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:39,537] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:39,537] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:39 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6da571917f4263a65ef70eba5be06a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190fb10bfa3a48fb95e13f25aca935cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 22it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:40,378] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 88 samples.\n",
      "[2022-10-02 11:15:40,378] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 88 samples.\n",
      "2022-10-02 11:15:40 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 88 samples.\n",
      "2022-10-02 11:15:40 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:40 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:40 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:40,388] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:40,388] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:40 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f11e0b03cffd46a3a693c65f5f676a56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0c80fd327534f358dd2e2d0572d3123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 22it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:41,688] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 89 samples.\n",
      "[2022-10-02 11:15:41,688] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 89 samples.\n",
      "2022-10-02 11:15:41 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 89 samples.\n",
      "2022-10-02 11:15:41 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:41 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:41 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:41,697] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:41,697] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:41 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e1d546612d4454b69ac8d6ba044a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80338829ef394bccb1e2e63703da6df9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 23it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:42,863] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 90 samples.\n",
      "[2022-10-02 11:15:42,863] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 90 samples.\n",
      "2022-10-02 11:15:42 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 90 samples.\n",
      "2022-10-02 11:15:42 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:42 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:42 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:42,874] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:42,874] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:42 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecf06e8e29e04a05ab11c4f85c2e298b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cdd3ad8ebeb4ba2882cbcb97354828c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 23it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:43,789] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 91 samples.\n",
      "[2022-10-02 11:15:43,789] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 91 samples.\n",
      "2022-10-02 11:15:43 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 91 samples.\n",
      "2022-10-02 11:15:43 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:43 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:43 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:43,796] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:43,796] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:43 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02971e98bd66459eae759af292e8cce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a962323d894d949b3a746af5b3112f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 23it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:44,675] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 92 samples.\n",
      "[2022-10-02 11:15:44,675] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 92 samples.\n",
      "2022-10-02 11:15:44 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 92 samples.\n",
      "2022-10-02 11:15:44 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:44 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:44 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:44,682] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:44,682] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:44 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c41b2eaabcc45d78ba194bc699d3887",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c06fa65efed4b6791e13c41f4c53b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 23it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:45,638] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 93 samples.\n",
      "[2022-10-02 11:15:45,638] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 93 samples.\n",
      "2022-10-02 11:15:45 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 93 samples.\n",
      "2022-10-02 11:15:45 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:45 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:45 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:45,648] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:45,648] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:45 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a474d50bddb4d748793922fe0e07c49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8ea96a5d3246a294b7705d96ef8084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 24it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:46,630] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 94 samples.\n",
      "[2022-10-02 11:15:46,630] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 94 samples.\n",
      "2022-10-02 11:15:46 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 94 samples.\n",
      "2022-10-02 11:15:46 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:46 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:46 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:46,637] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:46,637] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:46 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa05595b3d64c7eb7345426617745df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01938b632a704412b34bc02e2c63d6bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 24it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:47,563] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 95 samples.\n",
      "[2022-10-02 11:15:47,563] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 95 samples.\n",
      "2022-10-02 11:15:47 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 95 samples.\n",
      "2022-10-02 11:15:47 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:47 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:47 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:47,573] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:47,573] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:47 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8cc02fe69245dfa0950be2a4a6da86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a22e8106101403eb6fa0e6052c0b880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 24it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:48,444] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 96 samples.\n",
      "[2022-10-02 11:15:48,444] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 96 samples.\n",
      "2022-10-02 11:15:48 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 96 samples.\n",
      "2022-10-02 11:15:48 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:48 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:48 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:48,451] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:48,451] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:48 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013800ad12a44e2494bf3cdb734d0921",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3d73d26d48644d89d2839a3e22697b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 24it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:49,361] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 97 samples.\n",
      "[2022-10-02 11:15:49,361] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 97 samples.\n",
      "2022-10-02 11:15:49 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 97 samples.\n",
      "2022-10-02 11:15:49 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:49 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:49 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:49,370] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:49,370] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:49 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7a28379b6fe400f95de343b19809293",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc1ca80ae7248879a4b49c48d88a17e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 25it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:50,336] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 98 samples.\n",
      "[2022-10-02 11:15:50,336] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 98 samples.\n",
      "2022-10-02 11:15:50 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 98 samples.\n",
      "2022-10-02 11:15:50 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:50 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:50 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:50,349] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:50,349] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:50 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad48cb3e64464469badeb01cef354d4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cef1f82ae594e9a8713eb17f9698050",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 25it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:51,432] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 99 samples.\n",
      "[2022-10-02 11:15:51,432] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 99 samples.\n",
      "2022-10-02 11:15:51 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 99 samples.\n",
      "2022-10-02 11:15:51 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:51 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:51 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:51,444] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:51,444] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:51 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccee78d5c88144e0a6ecbd1aa4ea04dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad615c7334564c71a64347cdda6bd7f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 25it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:52,421] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 100 samples.\n",
      "[2022-10-02 11:15:52,421] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 100 samples.\n",
      "2022-10-02 11:15:52 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 100 samples.\n",
      "2022-10-02 11:15:52 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:52 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:52 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:52,429] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:52,429] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:52 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "814565c632224af79b433c16419bc40d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27bd214d884a41f9b32b82791d4e48f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 25it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:53,338] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 101 samples.\n",
      "[2022-10-02 11:15:53,338] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 101 samples.\n",
      "2022-10-02 11:15:53 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 101 samples.\n",
      "2022-10-02 11:15:53 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:53 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:53 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:53,344] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:53,344] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:53 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f15b11533a431c97843d05e88c93c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d620d90e207449f7a408b6c63f1b518a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 26it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:54,234] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 102 samples.\n",
      "[2022-10-02 11:15:54,234] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 102 samples.\n",
      "2022-10-02 11:15:54 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 102 samples.\n",
      "2022-10-02 11:15:54 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:54 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:54 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:54,241] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:54,241] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:54 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919737bdbebb440283f4519d7547a52a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625a1d1a5e714f49864a834e3749c82e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 26it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:55,154] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 103 samples.\n",
      "[2022-10-02 11:15:55,154] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 103 samples.\n",
      "2022-10-02 11:15:55 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 103 samples.\n",
      "2022-10-02 11:15:55 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:55 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:55 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:55,162] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:55,162] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:55 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85d8a7e9305d4a3597c9708187d64920",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e40e5227a84a481d932918515b620dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 26it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:56,152] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 104 samples.\n",
      "[2022-10-02 11:15:56,152] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 104 samples.\n",
      "2022-10-02 11:15:56 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 104 samples.\n",
      "2022-10-02 11:15:56 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:56 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:56 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:56,160] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:56,160] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:56 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cef3e50b1914edba00846e5a1556f65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98299021bf64dc49c8ba6a55eb6dcca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 26it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:57,290] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 105 samples.\n",
      "[2022-10-02 11:15:57,290] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 105 samples.\n",
      "2022-10-02 11:15:57 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 105 samples.\n",
      "2022-10-02 11:15:57 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:57 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:57 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:57,308] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:57,308] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:57 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f443177d4644b3addd177b8c565d44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7524bea61821495299a58cccb073fda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 27it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:58,617] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 106 samples.\n",
      "[2022-10-02 11:15:58,617] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 106 samples.\n",
      "2022-10-02 11:15:58 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 106 samples.\n",
      "2022-10-02 11:15:58 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:58 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:58 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:58,627] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:58,627] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:58 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dadeb3bd39e45ca9f8e2f6e1a2369b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f13def906f284e9d83ccba47b9b78a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 27it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:15:59,953] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 107 samples.\n",
      "[2022-10-02 11:15:59,953] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 107 samples.\n",
      "2022-10-02 11:15:59 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 107 samples.\n",
      "2022-10-02 11:15:59 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:15:59 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:15:59 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:15:59,964] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:15:59,964] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:15:59 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfa27172d99e4efc8f61c600ebf1cfad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc6b0f3deed4d6bb097aa88c8ab0806",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 27it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:01,090] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 108 samples.\n",
      "[2022-10-02 11:16:01,090] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 108 samples.\n",
      "2022-10-02 11:16:01 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 108 samples.\n",
      "2022-10-02 11:16:01 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:01 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:01 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:01,099] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:01,099] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:01 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59553baa56024171a1846db38f6221d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a951e5a57e49f894d0cebd3573d9cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 27it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:02,118] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 109 samples.\n",
      "[2022-10-02 11:16:02,118] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 109 samples.\n",
      "2022-10-02 11:16:02 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 109 samples.\n",
      "2022-10-02 11:16:02 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:02 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:02 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:02,129] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:02,129] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:02 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd0dd1b7182947beacf2a3c6faadb751",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6850fd84b78d4737be16d6c658b9dced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 28it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:03,263] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 110 samples.\n",
      "[2022-10-02 11:16:03,263] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 110 samples.\n",
      "2022-10-02 11:16:03 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 110 samples.\n",
      "2022-10-02 11:16:03 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:03 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:03 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:03,272] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:03,272] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:03 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18be7c9d16b5458c964ddf0811104c64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04fb2c26b3b64a8d95d32db69089ac3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 28it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:04,300] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 111 samples.\n",
      "[2022-10-02 11:16:04,300] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 111 samples.\n",
      "2022-10-02 11:16:04 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 111 samples.\n",
      "2022-10-02 11:16:04 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:04 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:04 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:04,313] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:04,313] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:04 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75fd462955d545c6894e4b8ff352bb03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a8dcb68c20f46fe829520f55f590d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 28it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:05,361] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 112 samples.\n",
      "[2022-10-02 11:16:05,361] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 112 samples.\n",
      "2022-10-02 11:16:05 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 112 samples.\n",
      "2022-10-02 11:16:05 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:05 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:05 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:05,369] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:05,369] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:05 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cb38a9b287e4cce91ea92986915ac49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9925e3b0b9d9443495a0ad6307e4e55f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 28it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:06,764] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 113 samples.\n",
      "[2022-10-02 11:16:06,764] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 113 samples.\n",
      "2022-10-02 11:16:06 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 113 samples.\n",
      "2022-10-02 11:16:06 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:06 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:06 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:06,781] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:06,781] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:06 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaad3aa5a149482db0cea8c8a9d45ec5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f12903a38bee4472b8b38c3583d4f970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 29it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:08,398] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 114 samples.\n",
      "[2022-10-02 11:16:08,398] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 114 samples.\n",
      "2022-10-02 11:16:08 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 114 samples.\n",
      "2022-10-02 11:16:08 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:08 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:08 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:08,409] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:08,409] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:08 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463e957a23c14221aa7dccd6400fafea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f34f91f4cdd9405a818de8f8307d0e8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 29it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:09,569] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 115 samples.\n",
      "[2022-10-02 11:16:09,569] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 115 samples.\n",
      "2022-10-02 11:16:09 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 115 samples.\n",
      "2022-10-02 11:16:09 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:09 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:09 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:09,577] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:09,577] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:09 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad4b12565f504536826cc01d6d56ece3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119e2613f6364fe4a3df226c610554fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 29it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:10,821] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 116 samples.\n",
      "[2022-10-02 11:16:10,821] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 116 samples.\n",
      "2022-10-02 11:16:10 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 116 samples.\n",
      "2022-10-02 11:16:10 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:10 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:10 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:10,832] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:10,832] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:10 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c54ab3f33b44668285a7bd22eb1e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f037aabf3c14b49a61a55a7b798d2aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 29it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:12,023] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 117 samples.\n",
      "[2022-10-02 11:16:12,023] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 117 samples.\n",
      "2022-10-02 11:16:12 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 117 samples.\n",
      "2022-10-02 11:16:12 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:12 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:12 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:12,032] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:12,032] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:12 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3a259f072242df958fd0e6cfc34953",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b90e19e09a334f73a68fb85376386b48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 30it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:13,106] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 118 samples.\n",
      "[2022-10-02 11:16:13,106] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 118 samples.\n",
      "2022-10-02 11:16:13 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 118 samples.\n",
      "2022-10-02 11:16:13 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:13 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:13 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:13,115] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:13,115] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:13 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27783d892427453290109cfc4f9d43f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90babdc86a7b460e90817f57015a3cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 30it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:14,183] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 119 samples.\n",
      "[2022-10-02 11:16:14,183] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 119 samples.\n",
      "2022-10-02 11:16:14 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 119 samples.\n",
      "2022-10-02 11:16:14 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:14 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:14 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:14,192] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:14,192] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:14 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de7b2e78c0e49628b418123437ab219",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa58f376dd4b4e03a56da09cfa75b0ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 30it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:15,303] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 120 samples.\n",
      "[2022-10-02 11:16:15,303] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 120 samples.\n",
      "2022-10-02 11:16:15 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 120 samples.\n",
      "2022-10-02 11:16:15 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:15 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:15 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:15,313] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:15,313] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:15 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1580795af02400cacd1eef1caa03a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f496c2927be546ebb63d471210320140",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 30it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:16,401] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 121 samples.\n",
      "[2022-10-02 11:16:16,401] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 121 samples.\n",
      "2022-10-02 11:16:16 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 121 samples.\n",
      "2022-10-02 11:16:16 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:16 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:16 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:16,410] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:16,410] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:16 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df0741ff614b4d5fb9cd1ad40ca401a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ceaf03b74cf475fbe4c77b992db8ec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 31it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:17,546] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 122 samples.\n",
      "[2022-10-02 11:16:17,546] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 122 samples.\n",
      "2022-10-02 11:16:17 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 122 samples.\n",
      "2022-10-02 11:16:17 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:17 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:17 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:17,554] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:17,554] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:17 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d0084b7606b4ca99828f2bda7256528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2741bf3f8bc466f9cfab0c31834e1ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 31it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:18,666] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 123 samples.\n",
      "[2022-10-02 11:16:18,666] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 123 samples.\n",
      "2022-10-02 11:16:18 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 123 samples.\n",
      "2022-10-02 11:16:18 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:18 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:18 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:18,674] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:18,674] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:18 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "729c9ac0a8374dfc844314abfc6dabea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39eef02aaae546b5ad932bdb9d37b907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 31it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:19,861] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 124 samples.\n",
      "[2022-10-02 11:16:19,861] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 124 samples.\n",
      "2022-10-02 11:16:19 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 124 samples.\n",
      "2022-10-02 11:16:19 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:19 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:19 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:19,870] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:19,870] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:19 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ecac067ff9438981da091eeadb629a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20ad40dee00045d1bc9bf1025ef865d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 31it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:21,062] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 125 samples.\n",
      "[2022-10-02 11:16:21,062] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 125 samples.\n",
      "2022-10-02 11:16:21 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 125 samples.\n",
      "2022-10-02 11:16:21 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:21 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:21 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:21,071] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:21,071] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:21 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02f09477044e4e5fafd37a44bbd497fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f71030458ff41bc8acd448c7b10272f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 32it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:22,254] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 126 samples.\n",
      "[2022-10-02 11:16:22,254] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 126 samples.\n",
      "2022-10-02 11:16:22 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 126 samples.\n",
      "2022-10-02 11:16:22 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:22 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:22 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:22,261] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:22,261] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:22 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c615b98088114f07ab56b455555dac35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1d7e0eae0946b18112749a1cfba814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 32it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:23,483] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 127 samples.\n",
      "[2022-10-02 11:16:23,483] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 127 samples.\n",
      "2022-10-02 11:16:23 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 127 samples.\n",
      "2022-10-02 11:16:23 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:23 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:23 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:23,492] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:23,492] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:23 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e8e81bf3154ffea818f6be0d4aadc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ce48468e4b24b2ba2ef5db40cf73e15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 32it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:24,652] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 128 samples.\n",
      "[2022-10-02 11:16:24,652] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 128 samples.\n",
      "2022-10-02 11:16:24 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 128 samples.\n",
      "2022-10-02 11:16:24 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:24 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:24 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:24,659] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:24,659] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:24 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c757344e5a141cba2f3b0de05dc3484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2125d8030d2f40818ef977a78bc7cbfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 32it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:25,833] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 129 samples.\n",
      "[2022-10-02 11:16:25,833] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 129 samples.\n",
      "2022-10-02 11:16:25 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 129 samples.\n",
      "2022-10-02 11:16:25 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:25 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:25 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:25,842] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:25,842] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:25 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc063db8ebb4d37a8d7fc915df1e37b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d4b18b561445f3a411a113e8c61be1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 33it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:27,000] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 130 samples.\n",
      "[2022-10-02 11:16:27,000] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 130 samples.\n",
      "2022-10-02 11:16:27 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 130 samples.\n",
      "2022-10-02 11:16:27 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:27 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:27 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:27,010] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:27,010] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:27 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ccfdce9184f4261927a42ec7d40aa06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b9ee6732c0848b080bb437afb7e6e85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 33it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:28,171] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 131 samples.\n",
      "[2022-10-02 11:16:28,171] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 131 samples.\n",
      "2022-10-02 11:16:28 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 131 samples.\n",
      "2022-10-02 11:16:28 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:28 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:28 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:28,181] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:28,181] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:28 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5d711adfe14a96a2dc3a58316feac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "786cff5e2f124ff8a0cac09aa66164f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 33it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:29,383] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 132 samples.\n",
      "[2022-10-02 11:16:29,383] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 132 samples.\n",
      "2022-10-02 11:16:29 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 132 samples.\n",
      "2022-10-02 11:16:29 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:29 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:29 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:29,391] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:29,391] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:29 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0b76abae8fc4e2285307ea5ba09e02e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "963e27a18ccf4c058defb06338b7679e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 33it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:30,572] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 133 samples.\n",
      "[2022-10-02 11:16:30,572] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 133 samples.\n",
      "2022-10-02 11:16:30 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 133 samples.\n",
      "2022-10-02 11:16:30 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:30 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:30 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:30,582] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:30,582] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:30 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e4e4a0559c6451f9f9647fe27da7d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c43e9fde9a5a408198ae509eabd5f39e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 34it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:31,820] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 134 samples.\n",
      "[2022-10-02 11:16:31,820] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 134 samples.\n",
      "2022-10-02 11:16:31 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 134 samples.\n",
      "2022-10-02 11:16:31 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:31 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:31 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:31,831] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:31,831] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:31 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef4e392e6b7460ba7b0e95436261b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b705935f4be420ea76374e70aafa7f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 34it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:33,058] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 135 samples.\n",
      "[2022-10-02 11:16:33,058] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 135 samples.\n",
      "2022-10-02 11:16:33 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 135 samples.\n",
      "2022-10-02 11:16:33 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:33 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:33 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:33,067] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:33,067] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:33 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12b341de09a3411494f94f15c8d2a8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b0a98c8e030415399e83cefe824ab6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 34it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:34,262] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 136 samples.\n",
      "[2022-10-02 11:16:34,262] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 136 samples.\n",
      "2022-10-02 11:16:34 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 136 samples.\n",
      "2022-10-02 11:16:34 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:34 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:34 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:34,271] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:34,271] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:34 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43bbfc15e78340fdbb4f91fd44446e06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cec878617524eaca1dd8d2c7ffba9a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 34it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:35,483] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 137 samples.\n",
      "[2022-10-02 11:16:35,483] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 137 samples.\n",
      "2022-10-02 11:16:35 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 137 samples.\n",
      "2022-10-02 11:16:35 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:35 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:35 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:35,490] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:35,490] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:35 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bba777469810431e8dda174d1c72bab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f4f24623404aa6b8f0612465c0ee37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 35it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:36,844] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 138 samples.\n",
      "[2022-10-02 11:16:36,844] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 138 samples.\n",
      "2022-10-02 11:16:36 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 138 samples.\n",
      "2022-10-02 11:16:36 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:36 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:36 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:36,855] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:36,855] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:36 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08455aed12ed4af8b01431763aaecfad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e541065a25c46a3b34f523784cd375a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 35it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:38,118] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 139 samples.\n",
      "[2022-10-02 11:16:38,118] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 139 samples.\n",
      "2022-10-02 11:16:38 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 139 samples.\n",
      "2022-10-02 11:16:38 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:38 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:38 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:38,127] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:38,127] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:38 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2493ab745544ba0af2bb9eb25353826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8fa91579c834f91859f1ab9559a8478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 35it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:39,386] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 140 samples.\n",
      "[2022-10-02 11:16:39,386] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 140 samples.\n",
      "2022-10-02 11:16:39 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 140 samples.\n",
      "2022-10-02 11:16:39 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:39 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:39 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:39,395] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:39,395] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:39 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585e1797211b4799ab92231c5bd1954c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9984af5560d34ca0b7e2a5e46d055ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 35it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:40,640] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 141 samples.\n",
      "[2022-10-02 11:16:40,640] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 141 samples.\n",
      "2022-10-02 11:16:40 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 141 samples.\n",
      "2022-10-02 11:16:40 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:40 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:40 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:40,648] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:40,648] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:40 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b562f6f04ede40f3b16b0a269c3dc536",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fe2ff49e264f3bb340f8dc1a3c7fb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 36it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:41,955] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 142 samples.\n",
      "[2022-10-02 11:16:41,955] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 142 samples.\n",
      "2022-10-02 11:16:41 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 142 samples.\n",
      "2022-10-02 11:16:41 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:41 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:41 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:41,966] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:41,966] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:41 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26ff1c4b8b0e4c47bc8df0c533ebcc65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "589a78f4da5c4216b5ba27e61d65b4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 36it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:43,273] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 143 samples.\n",
      "[2022-10-02 11:16:43,273] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 143 samples.\n",
      "2022-10-02 11:16:43 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 143 samples.\n",
      "2022-10-02 11:16:43 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:43 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:43 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:43,281] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:43,281] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:43 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b476045c73784560a1f309b0d0d18150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcaec6a174664d4aa81ef8d1209c2804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 36it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:44,593] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 144 samples.\n",
      "[2022-10-02 11:16:44,593] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 144 samples.\n",
      "2022-10-02 11:16:44 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 144 samples.\n",
      "2022-10-02 11:16:44 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:44 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:44 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:44,601] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:44,601] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:44 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08e6bb724b604e2ca42a17aee7c7c3bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8e109fdbf2495da54da56478ae24a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 36it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:45,936] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 145 samples.\n",
      "[2022-10-02 11:16:45,936] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 145 samples.\n",
      "2022-10-02 11:16:45 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 145 samples.\n",
      "2022-10-02 11:16:45 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:45 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:45 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:45,944] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:45,944] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:45 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7c89670dc824e078369389c33b268fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dced8163f754158a9b04999613a0143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 37it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:47,255] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 146 samples.\n",
      "[2022-10-02 11:16:47,255] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 146 samples.\n",
      "2022-10-02 11:16:47 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 146 samples.\n",
      "2022-10-02 11:16:47 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:47 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:47 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:47,265] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:47,265] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:47 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b73db3542f74e149cf7b34a92104bc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1e6313b7ee414292de16d53058655d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 37it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:48,560] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 147 samples.\n",
      "[2022-10-02 11:16:48,560] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 147 samples.\n",
      "2022-10-02 11:16:48 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 147 samples.\n",
      "2022-10-02 11:16:48 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:48 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:48 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:48,571] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:48,571] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:48 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0c742ba35f40cf88863d66681e58f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e6c75fecac4a0bb6217cf944bc8d0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 37it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:49,867] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 148 samples.\n",
      "[2022-10-02 11:16:49,867] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 148 samples.\n",
      "2022-10-02 11:16:49 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 148 samples.\n",
      "2022-10-02 11:16:49 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:49 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:49 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:49,876] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:49,876] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:49 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03b8e81098234f73bfed2d73d0b20a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508a89382d35412b90d0d0e081ba612c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 37it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:51,230] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 149 samples.\n",
      "[2022-10-02 11:16:51,230] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 149 samples.\n",
      "2022-10-02 11:16:51 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 149 samples.\n",
      "2022-10-02 11:16:51 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:51 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:51 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:51,239] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:51,239] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:51 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d23f486d2104782b861e6b4c3c92ec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec81d3c5b64a4f6982d5ba7465c24527",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 38it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:53,225] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 150 samples.\n",
      "[2022-10-02 11:16:53,225] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 150 samples.\n",
      "2022-10-02 11:16:53 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 150 samples.\n",
      "2022-10-02 11:16:53 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:53 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:53 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:53,233] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:53,233] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:53 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4071b614f78472ca861cea9f93e365d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d3d3f2032ff409c98ed67aa66dbfb19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 38it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:54,566] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 151 samples.\n",
      "[2022-10-02 11:16:54,566] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 151 samples.\n",
      "2022-10-02 11:16:54 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 151 samples.\n",
      "2022-10-02 11:16:54 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:54 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:54 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:54,576] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:54,576] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:54 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0966436abe3c4582bac73fe9a1c60a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7890b106125d4ec69f1cafa86bbd0e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 38it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:55,995] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 152 samples.\n",
      "[2022-10-02 11:16:55,995] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 152 samples.\n",
      "2022-10-02 11:16:55 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 152 samples.\n",
      "2022-10-02 11:16:56 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:56 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:56 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:56,005] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:56,005] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:56 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482254a4763148ffb5cb25b01b225241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3ad2ca22ea84d339aee2640efab9ce9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 38it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:57,413] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 153 samples.\n",
      "[2022-10-02 11:16:57,413] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 153 samples.\n",
      "2022-10-02 11:16:57 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 153 samples.\n",
      "2022-10-02 11:16:57 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:57 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:57 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:57,422] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:57,422] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:57 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e2bfb633e0e491a94b19095ca4b3601",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd535c6a35bd4d6db4528a94c2227e03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 39it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:16:58,825] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 154 samples.\n",
      "[2022-10-02 11:16:58,825] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 154 samples.\n",
      "2022-10-02 11:16:58 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 154 samples.\n",
      "2022-10-02 11:16:58 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:16:58 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:16:58 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:16:58,834] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:16:58,834] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:16:58 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ba723ad91ef49ec80788aea4390111e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0f8b285da74ef0af4b8158c4c5ee90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 39it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:00,220] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 155 samples.\n",
      "[2022-10-02 11:17:00,220] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 155 samples.\n",
      "2022-10-02 11:17:00 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 155 samples.\n",
      "2022-10-02 11:17:00 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:00 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:00 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:00,229] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:00,229] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:00 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b365cf99055446c986bd4915d099871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "030b38f856ef4653b2d3eee0e9c90c86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 39it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:01,605] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 156 samples.\n",
      "[2022-10-02 11:17:01,605] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 156 samples.\n",
      "2022-10-02 11:17:01 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 156 samples.\n",
      "2022-10-02 11:17:01 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:01 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:01 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:01,614] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:01,614] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:01 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c000c2bb375404dae887f73e978f721",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "074b0137bd0b468a8322eddb124a1a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 39it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:03,094] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 157 samples.\n",
      "[2022-10-02 11:17:03,094] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 157 samples.\n",
      "2022-10-02 11:17:03 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 157 samples.\n",
      "2022-10-02 11:17:03 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:03 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:03 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:03,103] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:03,103] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:03 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9b6c08a7abe48a88639bdc579517dc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d4924713f2f4c528980de9974e18729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 40it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:04,502] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 158 samples.\n",
      "[2022-10-02 11:17:04,502] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 158 samples.\n",
      "2022-10-02 11:17:04 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 158 samples.\n",
      "2022-10-02 11:17:04 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:04 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:04 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:04,509] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:04,509] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:04 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3489c0c4fe2846329d39cea538f6617b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ce6174256445e48339ee1146eb6d2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 40it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:05,924] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 159 samples.\n",
      "[2022-10-02 11:17:05,924] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 159 samples.\n",
      "2022-10-02 11:17:05 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 159 samples.\n",
      "2022-10-02 11:17:05 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:05 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:05 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:05,936] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:05,936] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:05 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ceba8a18a9c43329e6c14adf16a4153",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e12c06e08e46496c9e563f8b7269f286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 40it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:07,460] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 160 samples.\n",
      "[2022-10-02 11:17:07,460] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 160 samples.\n",
      "2022-10-02 11:17:07 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 160 samples.\n",
      "2022-10-02 11:17:07 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:07 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:07 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:07,471] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:07,471] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:07 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f962dbb5938b4e10998f112847ce967e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7516e8050f1c4813beb9f2e2b682b154",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 40it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:09,091] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 161 samples.\n",
      "[2022-10-02 11:17:09,091] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 161 samples.\n",
      "2022-10-02 11:17:09 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 161 samples.\n",
      "2022-10-02 11:17:09 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:09 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:09 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:09,101] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:09,101] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:09 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "130\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9512064dd9e749a2a1e11b8132bada9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d8876ccc523410e921d247aed1d0f1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 41it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:10,614] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 162 samples.\n",
      "[2022-10-02 11:17:10,614] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 162 samples.\n",
      "2022-10-02 11:17:10 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 162 samples.\n",
      "2022-10-02 11:17:10 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:10 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:10 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:10,624] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:10,624] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:10 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff95842cfea408a967f60ec693b6cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3bd12c34164894b79d2745d303b1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 41it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:12,110] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 163 samples.\n",
      "[2022-10-02 11:17:12,110] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 163 samples.\n",
      "2022-10-02 11:17:12 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 163 samples.\n",
      "2022-10-02 11:17:12 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:12 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:12 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:12,118] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:12,118] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:12 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46c549ca79fc49e79f3896b952ac457d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c28a4af03ffa43a897031766ccbd300c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 41it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:13,600] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 164 samples.\n",
      "[2022-10-02 11:17:13,600] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 164 samples.\n",
      "2022-10-02 11:17:13 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 164 samples.\n",
      "2022-10-02 11:17:13 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:13 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:13 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:13,608] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:13,608] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:13 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9460abbec4f44869a31a0f12de5e8e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f8f50999e8418f94c156775953c158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 41it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:15,059] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 165 samples.\n",
      "[2022-10-02 11:17:15,059] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 165 samples.\n",
      "2022-10-02 11:17:15 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 165 samples.\n",
      "2022-10-02 11:17:15 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:15 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:15 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:15,068] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:15,068] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:15 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff6d430578494614b3fa91c431e94c39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1712b8cb4034042a80e070d5e03a9bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 42it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:16,556] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 166 samples.\n",
      "[2022-10-02 11:17:16,556] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 166 samples.\n",
      "2022-10-02 11:17:16 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 166 samples.\n",
      "2022-10-02 11:17:16 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:16 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:16 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:16,566] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:16,566] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:16 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09601b439aa84f21a56400b9a9094290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68592938c31742d4b1b0f601b46e0c8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 42it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:18,152] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 167 samples.\n",
      "[2022-10-02 11:17:18,152] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 167 samples.\n",
      "2022-10-02 11:17:18 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 167 samples.\n",
      "2022-10-02 11:17:18 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:18 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:18 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:18,162] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:18,162] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:18 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd14927c2a1d403697ff7ee9a28595ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "197f4b507e0b4fce9858336d983da68e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 42it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:19,640] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 168 samples.\n",
      "[2022-10-02 11:17:19,640] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 168 samples.\n",
      "2022-10-02 11:17:19 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 168 samples.\n",
      "2022-10-02 11:17:19 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:19 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:19 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:19,648] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:19,648] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:19 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0055263614f64a2d97e038b57f56d7dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9041fc791b0f48938337bd9c19e17229",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 42it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:21,175] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 169 samples.\n",
      "[2022-10-02 11:17:21,175] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 169 samples.\n",
      "2022-10-02 11:17:21 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 169 samples.\n",
      "2022-10-02 11:17:21 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:21 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:21 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:21,182] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:21,182] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:21 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fa0d0bf5a3c4ade97b59ee00fc3f841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56c071aec1844186bfdb15bf1c246f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 43it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:22,770] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 170 samples.\n",
      "[2022-10-02 11:17:22,770] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 170 samples.\n",
      "2022-10-02 11:17:22 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 170 samples.\n",
      "2022-10-02 11:17:22 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:22 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:22 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:22,778] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:22,778] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:22 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c458a14d865143eaa51c7b08c5f81a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43aa8c378aa74f249088011d242f796d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 43it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:24,275] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 171 samples.\n",
      "[2022-10-02 11:17:24,275] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 171 samples.\n",
      "2022-10-02 11:17:24 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 171 samples.\n",
      "2022-10-02 11:17:24 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:24 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:24 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:24,284] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:24,284] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:24 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658ebb93fb4649ccb9202a56a2b3a49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b1cdf0a0a364b658e61d8538f1ff0a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 43it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:25,801] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 172 samples.\n",
      "[2022-10-02 11:17:25,801] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 172 samples.\n",
      "2022-10-02 11:17:25 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 172 samples.\n",
      "2022-10-02 11:17:25 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:25 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:25 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:25,810] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:25,810] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:25 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6043d4674f8940fc832a98e9349c9e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "190f8f5cc79c4540a3438a0d66022c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 43it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:27,421] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 173 samples.\n",
      "[2022-10-02 11:17:27,421] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 173 samples.\n",
      "2022-10-02 11:17:27 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 173 samples.\n",
      "2022-10-02 11:17:27 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:27 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:27 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:27,429] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:27,429] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:27 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3f1ecf25fa41dbac3be7a44d6ec988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bd7e4a0d8243168acc73a4f4f484d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 44it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:29,133] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 174 samples.\n",
      "[2022-10-02 11:17:29,133] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 174 samples.\n",
      "2022-10-02 11:17:29 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 174 samples.\n",
      "2022-10-02 11:17:29 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:29 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:29 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:29,141] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:29,141] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:29 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42af5a93111d4ac89db54c4c06387844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2819c244cf4fec83f757adf0640f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 44it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:30,769] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 175 samples.\n",
      "[2022-10-02 11:17:30,769] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 175 samples.\n",
      "2022-10-02 11:17:30 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 175 samples.\n",
      "2022-10-02 11:17:30 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:30 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:30 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:30,781] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:30,781] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:30 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ecda51495f4effb31cb0511b41b947",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de03364429574ed19cec2dd55c8616ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 44it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:32,486] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 176 samples.\n",
      "[2022-10-02 11:17:32,486] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 176 samples.\n",
      "2022-10-02 11:17:32 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 176 samples.\n",
      "2022-10-02 11:17:32 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:32 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:32 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:32,494] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:32,494] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:32 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb285167b584f62bb1d51be17dd902c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d29b4fc86e54222b6cc3b7c5ade7a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 44it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:34,071] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 177 samples.\n",
      "[2022-10-02 11:17:34,071] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 177 samples.\n",
      "2022-10-02 11:17:34 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 177 samples.\n",
      "2022-10-02 11:17:34 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:34 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:34 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:34,080] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:34,080] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:34 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ee1b3d3c2ec4c1c9593aeab67483b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "095fe7fe14794c879906e7de0b6dcb82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 45it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:35,720] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 178 samples.\n",
      "[2022-10-02 11:17:35,720] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 178 samples.\n",
      "2022-10-02 11:17:35 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 178 samples.\n",
      "2022-10-02 11:17:35 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:35 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:35 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:35,728] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:35,728] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:35 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "147\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d1f49d823cb4a089297e3f51c9de188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b0c91d3d93418d9048772cf0764cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 45it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:37,395] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 179 samples.\n",
      "[2022-10-02 11:17:37,395] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 179 samples.\n",
      "2022-10-02 11:17:37 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 179 samples.\n",
      "2022-10-02 11:17:37 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:37 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:37 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:37,401] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:37,401] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:37 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "148\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14c812e9143d4908b40ec23840e002fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37ba4a688be421b81b46f4dbd0cc632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 45it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:39,004] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 180 samples.\n",
      "[2022-10-02 11:17:39,004] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 180 samples.\n",
      "2022-10-02 11:17:39 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 180 samples.\n",
      "2022-10-02 11:17:39 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:39 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:39 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:39,016] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:39,016] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:39 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1cc034e74d7d4aba9ad650873e0f0ff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8f01a7bc1d4e15a4be4958ea685d5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 45it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:40,680] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 181 samples.\n",
      "[2022-10-02 11:17:40,680] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 181 samples.\n",
      "2022-10-02 11:17:40 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 181 samples.\n",
      "2022-10-02 11:17:40 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:40 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:40 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:40,690] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:40,690] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:40 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d6354f05304b9cac82db8fd72a0766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e329acee428440ca6c40cd84a0cec1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 46it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:42,376] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 182 samples.\n",
      "[2022-10-02 11:17:42,376] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 182 samples.\n",
      "2022-10-02 11:17:42 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 182 samples.\n",
      "2022-10-02 11:17:42 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:42 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:42 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:42,385] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:42,385] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:42 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be209ede3ef84b26a4d310c8dc41dfb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4540e2362274f0fb3c759e24b673c70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 46it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:44,047] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 183 samples.\n",
      "[2022-10-02 11:17:44,047] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 183 samples.\n",
      "2022-10-02 11:17:44 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 183 samples.\n",
      "2022-10-02 11:17:44 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:44 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:44 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:44,056] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:44,056] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:44 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff2c0687537f45d190ee658c514aa7ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9917fc70eec462c93d309c0e05afbdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 46it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:45,693] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 184 samples.\n",
      "[2022-10-02 11:17:45,693] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 184 samples.\n",
      "2022-10-02 11:17:45 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 184 samples.\n",
      "2022-10-02 11:17:45 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:45 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:45 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:45,700] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:45,700] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:45 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896672a5b37c424b9c270f2b5a272b97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d96e1e0b98fe4efca9fa18e951c33629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 46it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:47,424] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 185 samples.\n",
      "[2022-10-02 11:17:47,424] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 185 samples.\n",
      "2022-10-02 11:17:47 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 185 samples.\n",
      "2022-10-02 11:17:47 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:47 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:47 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:47,434] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:47,434] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:47 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d79b7e2f80a44c7dadbff666e9eaa035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ea450cd968248b6b684c8b26c7526f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 47it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:49,133] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 186 samples.\n",
      "[2022-10-02 11:17:49,133] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 186 samples.\n",
      "2022-10-02 11:17:49 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 186 samples.\n",
      "2022-10-02 11:17:49 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:49 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:49 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:49,141] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:49,141] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:49 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d3c1829d3d0490284a9d7aa4004c565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ceaaa80eac941798a040a4b6f14192e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 47it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:50,840] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 187 samples.\n",
      "[2022-10-02 11:17:50,840] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 187 samples.\n",
      "2022-10-02 11:17:50 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 187 samples.\n",
      "2022-10-02 11:17:50 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:50 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:50 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:50,850] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:50,850] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:50 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "156\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb080b2dab94cf7b6ba8aa7eef007e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "945121e57a844462a7d1b9d89419cbc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 47it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:52,531] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 188 samples.\n",
      "[2022-10-02 11:17:52,531] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 188 samples.\n",
      "2022-10-02 11:17:52 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 188 samples.\n",
      "2022-10-02 11:17:52 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:52 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:52 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:52,539] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:52,539] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:52 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1b00c270a5403b85acfc2393401d8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044d0c9e1fa5456187f0f15e9127a578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 47it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:54,361] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 189 samples.\n",
      "[2022-10-02 11:17:54,361] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 189 samples.\n",
      "2022-10-02 11:17:54 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 189 samples.\n",
      "2022-10-02 11:17:54 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:54 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:54 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:54,370] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:54,370] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:54 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b0f0c2e414846feb45eccc7e921d2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02824cf8a1f64c79889454b6fa50d889",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:56,177] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:17:56,177] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:17:56 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:17:56 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:56 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:56 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:56,186] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:56,186] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:56 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09212d5d65004b249f3fa4e37cbed221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f16769182b3497fbd40bc32dc2fa728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:57,876] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:17:57,876] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:17:57 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:17:57 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:57 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:57 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:57,883] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:57,883] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:57 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b22e978256114c79a229549591bed934",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c18ed9c789648959912c3206deb5c2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:17:59,631] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:17:59,631] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:17:59 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:17:59 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:17:59 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:17:59 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:17:59,640] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:17:59,640] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:17:59 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40832450a7d1431587430bb5fe4f1e7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05da9a7cc9f942c0b317695f5c981573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:01,558] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:01,558] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:01 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:01 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:01 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:01 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:01,566] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:01,566] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:01 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f585260b70645b2ae3e21cf347af0d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d228259162114a35824a5ddc334be312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:03,303] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:03,303] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:03 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:03 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:03 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:03 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:03,313] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:03,313] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:03 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95560ee00a92427bb76d1847c235adfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9595d5b0344b4b578f911f50f25eadcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:05,121] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:05,121] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:05 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:05 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:05 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:05 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:05,130] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:05,130] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:05 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8a486dd13eb4eac9e38f49252f75004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48a99854c073477fb7838a5a6dede4b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:06,824] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:06,824] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:06 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:06 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:06 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:06 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:06,837] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:06,837] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:06 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "165\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22f86b86ac534c43ad91d741ed8cd12c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bed5c9a600da48378604e150db180465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:08,523] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:08,523] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:08 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:08 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:08 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:08 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:08,531] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:08,531] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:08 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8bd55def694e12a3d5efe96e826a69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9266452c1fd84b56ab620e9dc541df62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:10,298] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:10,298] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:10 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:10 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:10 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:10 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:10,309] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:10,309] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:10 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d42b3b8c52a4e329a57b67fe52d65dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51a2bb562794aeeb558148f4918c898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:12,060] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:12,060] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:12 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:12 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:12 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:12 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:12,071] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:12,071] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:12 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7af9385e7644e579737570b0d0e7e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05907e3b05434508957a16378c87d5d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:13,970] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:13,970] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:13 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:13 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:13 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:13 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:13,978] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:13,978] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:13 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6ea8d961c0d4b438ff7705ead409b24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb620bfdd24491bb101a75b95e368fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:15,693] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:15,693] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:15 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:15 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:15 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:15 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:15,705] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:15,705] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:15 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "541ec0870afa4b7e91ec13362f565a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f58c180d5c4343cba7b349e53194d89f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:17,410] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:17,410] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:17 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:17 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:17 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:17 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:17,418] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:17,418] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:17 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "938940b84e02455dbe4d97e566ce6b5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb1073e273d742e1af3d79e073b52b3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:19,173] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:19,173] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:19 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:19 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:19 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:19 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:19,180] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:19,180] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:19 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614bcdc0d8ba4492af1f7e0e91750082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d99b6d0743b64edf863a4d3b77e51722",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:20,879] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:20,879] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:20 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:20 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:20 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:20 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:20,887] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:20,887] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:20 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b9205381274498959d89f8ff3e75d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ffc9b0cba484307a3c5bbc92bd44b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:22,636] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:22,636] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:22 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:22 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:22 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:22 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:22,644] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:22,644] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:22 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "328b81f0a6ad4544badd7dfe9327d528",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97232e4698af4e06b2802e9f0c64c7c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:24,314] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:24,314] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:24 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:24 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:24 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:24 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:24,321] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:24,321] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:24 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f731c522f54b048250f841dae07171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e446b5d5c54515a5bb1e4c241db594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:26,032] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:26,032] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:26 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:26 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:26 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:26 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:26,041] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:26,041] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:26 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28fcaf14375e490a8455dd9418b54131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dca6f53c58348f2bcf83fa16d17503f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:27,763] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:27,763] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:27 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:27 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:27 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:27 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:27,772] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:27,772] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:27 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45d3e75085684a81b6dc40f5c983a36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08603ea99e8e4b81bc4b780579169028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:29,449] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:29,449] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:29 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:29 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:29 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:29 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:29,456] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:29,456] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:29 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbdebec8f1724bd9aeaf0d305a81ac1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb2a01ac06974f8aa89edbc438669521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:31,215] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:31,215] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:31 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:31 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:31 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:31 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:31,223] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:31,223] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:31 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92484f62bf0343fca79531105ff41c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f401c733474e6b9575dcaa03a5bb50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:32,973] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:32,973] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:32 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:32 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:32 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:32 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:32,980] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:32,980] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:32 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "180\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7abcbe452ecc4ec586524b0edfb9b0d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f8539edbb6477a88d3b99398965fcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:34,714] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:34,714] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:34 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:34 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:34 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:34 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:34,722] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:34,722] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:34 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "044427f133ce4cf392929b03b92ed250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab91d91adb74c82b44237adea2321ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:36,553] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:36,553] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:36 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:36 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:36 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:36 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:36,563] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:36,563] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:36 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "612234f386c646ffbb64be4ae75b96db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc8cafdbdbb04633a9ee8599e44390a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:38,272] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:38,272] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:38 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:38 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:38 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:38 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:38,281] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:38,281] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:38 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8dbff65bac443ba304d18a546462f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c38e153fd8b4c7686e6b191937d339f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:40,058] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:40,058] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:40 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:40 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:40 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:40 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:40,066] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:40,066] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:40 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e242cc16b4494e8c9e690b614203d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c41e79dcaa45bcbcbf3221d484c331",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:41,760] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:41,760] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:41 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:41 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:41 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:41 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:41,769] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:41,769] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:41 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a115ed2df87425f84a9701551b1bcbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dcf39d6040345b4b1b84a2543dfb0b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:43,546] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:43,546] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:43 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:43 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:43 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:43 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:43,559] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:43,559] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:43 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "186\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f94dcd611c4c2b971aba456e3cc23a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6fbc1448804d9baefaebad4ed90354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:45,206] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:45,206] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:45 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:45 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:45 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:45 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:45,215] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:45,215] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:45 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7249876b6fb34e3b9c6264dd833a0f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040bf82be5604918aa888d1da06492bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:46,963] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:46,963] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:46 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:46 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:46 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:46 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:46,974] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:46,974] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:46 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f5aecc7ddfb4be8b7bc6fd51d94a740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de58d75027be4b61858bd2f14d5b55f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:48,640] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:48,640] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:48 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:48 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:48 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:48 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:48,650] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:48,650] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:48 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "189\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bda704a239004821b943347fd8811aed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b97fad0d6d48768e21a45f2a2b3a67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:50,369] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:50,369] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:50 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:50 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:50 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:50 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:50,377] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:50,377] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:50 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13365ff7c21e4bb48d4b6b04b733eac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2504f04060b146fbaf20fcd78b2d5405",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:52,143] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:52,143] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:52 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:52 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:52 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:52 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:52,153] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:52,153] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:52 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "191\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1bdae7822f4ac9a9e0e05312c03ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea909492d7646028c47b36f55db15c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:53,844] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:53,844] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:53 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:53 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:53 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:53 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:53,853] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:53,853] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:53 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6c3f58b0b434072a42b2dc5375397ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4acb1e3e5724831b2da45a9152e43e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:55,566] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:55,566] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:55 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:55 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:55 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:55 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:55,577] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:55,577] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:55 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4a22a31c3947179cda96cfa2bfd6a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba2b7a8ee19f42ee8073a3b32d555ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:57,282] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:57,282] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:57 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:57 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:57 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:57 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:57,291] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:57,291] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:57 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8cfe36ac94047e0890773cd541979f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e68f3d7928341a8886c95bd7e1e4ece",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:18:59,033] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:18:59,033] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:59 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:18:59 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:18:59 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:18:59 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:18:59,042] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:18:59,042] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:18:59 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44773e5943e547a0992d7bc0b233c002",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "908a95d5eaf34f988a25b05c29936959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:00,735] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:00,735] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:00 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:00 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:00 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:00 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:00,742] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:00,742] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:00 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df2171b2b8e4142aa6c7bd3a22b925f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9555c3ae8c804c0bb0b3ad5cb6d1f084",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:02,507] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:02,507] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:02 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:02 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:02 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:02 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:02,516] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:02,516] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:02 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956881696c4048ceb3451e3c77d5fef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0beb412769e543ceab864662613a60a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:04,248] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:04,248] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:04 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:04 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:04 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:04 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:04,257] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:04,257] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:04 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5489abed6e3e485fb62339647f363f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cccc9b2a2d14800be18f4d2273882e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:05,991] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:05,991] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:05 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:05 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:05 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:05 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:05,999] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:05,999] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:05 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187860002d764da08080c1ab8c96a38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1fb3d0f12449e78fef318d9eaa899a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:07,706] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:07,706] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:07 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:07 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:07 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:07 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:07,716] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:07,716] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:07 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5192fce6ee6c4511be7a538afaa12d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fccee1dca7f0474fbd1335cecd4dff74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:09,491] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:09,491] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:09 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:09 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:09 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:09 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:09,502] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:09,502] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:09 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09ebb9cb56147c1a9e5949d8b9ba8ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f789f188c064682afd66df92b54eec2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:11,219] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:11,219] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:11 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:11 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:11 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:11 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:11,227] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:11,227] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:11 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "299263032be04b728190b241ecc829d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cec0a97013349f3923d7d15a37749fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:12,954] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:12,954] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:12 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:12 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:12 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:12 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:12,962] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:12,962] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:12 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f76990f32ea4a86959c7703f804b336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4367e2b038ba4317895c76b0a83a0bcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:14,684] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:14,684] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:14 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:14 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:14 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:14 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:14,694] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:14,694] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:14 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50fea0fdd8d6442bbcb20dce6af6ecf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0d6f91c85a4b40a3f5d6b2fe469cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:16,482] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:16,482] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:16 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:16 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:16 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:16 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:16,492] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:16,492] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:16 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6efa75e023054be9a055156cb856eb0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18c7ca534ae344d2b60ec45f98a0aaeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:18,280] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:18,280] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:18 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:18 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:18 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:18 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:18,289] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:18,289] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:18 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7623b2793b034a0794c0b76f99e8e2f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6333e502a607462facd9d1eb7fc6b6ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:20,036] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:20,036] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:20 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:20 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:20 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:20 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:20,046] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:20,046] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:20 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2014eca7d9c14743884083cb47c6ddc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "630f7d81b4e24c4dac512035678d2ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:21,830] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:21,830] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:21 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:21 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:21 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:21 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:21,838] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:21,838] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:21 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c899f956b3644d36a1674185efdaf1b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be897b351f3493b852f7c315e7d6ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:23,545] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:23,545] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:23 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:23 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:23 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:23 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:23,554] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:23,554] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:23 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff516fff0394247b0dadc351349b306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2fe3cf7abb04aff822e90b5fec4e21a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:25,334] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:25,334] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:25 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:25 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:25 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:25 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:25,343] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:25,343] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:25 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "423acdbfddac4211acf0a23a933e266c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f7df88de52144ef9633032d3dc95239",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:27,132] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:27,132] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:27 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:27 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:27 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:27 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:27,141] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:27,141] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:27 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4cbdbd2e214aa8b254921f102335f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54d025c50f1d411a931547061b47dbd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:28,884] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:28,884] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:28 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:28 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:28 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:28 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:28,892] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:28,892] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:28 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70f69c78e7574e07b54f8e886e087834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f22615c724314bf5be75ef48153b6c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:30,642] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:30,642] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:30 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:30 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:30 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:30 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:30,654] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:30,654] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:30 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7539767bc64c5fa36a091869590be0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f50d56557d14ae89609a92a87b6ac98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:32,462] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:32,462] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:32 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:32 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:32 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:32 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:32,472] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:32,472] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:32 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5791a48beae84313b93c8d428dfcc09d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b094f722714f03ab8a7bc8fbc26098",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:34,165] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:34,165] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:34 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:34 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:34 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:34 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:34,175] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:34,175] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:34 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "215\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b89d1800c24c468dfcee9192b4533c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f9364ad0e8442fb3471d9aea4941d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:35,931] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:35,931] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:35 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:35 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:35 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:35 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:35,939] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:35,939] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:35 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10bd0aa375b249c2a65a76c02df6d9f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abce261e38304bc99b6bbe2635b3e255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:37,643] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:37,643] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:37 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:37 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:37 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:37 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:37,651] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:37,651] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:37 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21745439ef74c03aa532d9d89a99aa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2c9af59e0b436c975f035821682f13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:39,366] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:39,366] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:39 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:39 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:39 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:39 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:39,375] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:39,375] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:39 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ea295969964eee84be83ca50c1673b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d073d5617d0a44bda53d7629ddc28e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:41,069] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:41,069] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:41 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:41 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:41 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:41 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:41,077] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:41,077] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:41 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d85b9f2f21cc46d38d84e3d9a0018b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a108bae09d164a179892cae9677e05e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:42,805] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:42,805] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:42 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:42 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:42 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:42 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:42,813] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:42,813] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:42 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba68dbb0b87b417b9f3c0a54be70e7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7cfcd7d7cc04810a4f22252749f75db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:44,536] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:44,536] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:44 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:44 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:44 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:44 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:44,544] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:44,544] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:44 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "020263201f3748b485e63c96353a22ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "330735e3566f49c28c6db40ab86f0622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: 48it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-10-02 11:19:46,263] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "[2022-10-02 11:19:46,263] INFO | darts.models.forecasting.torch_forecasting_model | Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:46 darts.models.forecasting.torch_forecasting_model INFO: Train dataset contains 190 samples.\n",
      "2022-10-02 11:19:46 pytorch_lightning.utilities.distributed INFO: GPU available: False, used: False\n",
      "2022-10-02 11:19:46 pytorch_lightning.utilities.distributed INFO: TPU available: False, using: 0 TPU cores\n",
      "2022-10-02 11:19:46 pytorch_lightning.utilities.distributed INFO: IPU available: False, using: 0 IPUs\n",
      "[2022-10-02 11:19:46,272] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "[2022-10-02 11:19:46,272] WARNING | darts.models.forecasting.torch_forecasting_model | Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n",
      "2022-10-02 11:19:46 darts.models.forecasting.torch_forecasting_model WARNING: Attempting to retrain the model without resuming from a checkpoint. This is currently discouraged. Consider setting `save_checkpoints` to `True` and specifying `model_name` at model creation. Then call `model = TFTModel.load_from_checkpoint(model_name, best=False)`. Finally, train the model with `model.fit(..., epochs=new_epochs)` where `new_epochs` is the sum of (epochs already trained + some additional epochs).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "665bde7c59e94fc7bbee6a9907e8e4ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_prediction = []\n",
    "for i, data in enumerate(test_time_series):\n",
    "    # predict    \n",
    "    y_approx = best_model.predict(1)\n",
    "    test_prediction.append(y_approx)\n",
    "    print(i)\n",
    "    # use test set prior to this step to retrain model\n",
    "    best_model.fit(test_time_series[:i+best_model.input_chunk_length+best_model.input_chunk_length], epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "dd6c11a6-57e8-43b2-9eb1-f4190f992174",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T10:19:47.911839Z",
     "iopub.status.busy": "2022-10-02T10:19:47.911610Z",
     "iopub.status.idle": "2022-10-02T10:19:47.916501Z",
     "shell.execute_reply": "2022-10-02T10:19:47.915544Z",
     "shell.execute_reply.started": "2022-10-02T10:19:47.911808Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "223"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "9931ecb9-c56c-4628-8184-198b5eadda3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T11:00:02.209862Z",
     "iopub.status.busy": "2022-10-02T11:00:02.209379Z",
     "iopub.status.idle": "2022-10-02T11:00:02.242056Z",
     "shell.execute_reply": "2022-10-02T11:00:02.240973Z",
     "shell.execute_reply.started": "2022-10-02T11:00:02.209801Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_prediction_ls = [pred.values() for pred in test_prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0b4f3540-c0ed-4660-be93-e3346468e647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T11:09:33.273403Z",
     "iopub.status.busy": "2022-10-02T11:09:33.273123Z",
     "iopub.status.idle": "2022-10-02T11:09:33.278960Z",
     "shell.execute_reply": "2022-10-02T11:09:33.277415Z",
     "shell.execute_reply.started": "2022-10-02T11:09:33.273372Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = mm_scaler.inverse_transform(np.array(test_prediction_ls).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8ec871b3-105f-4dcb-a290-c0e290349580",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T12:16:21.788949Z",
     "iopub.status.busy": "2022-10-02T12:16:21.788589Z",
     "iopub.status.idle": "2022-10-02T12:16:22.487191Z",
     "shell.execute_reply": "2022-10-02T12:16:22.486450Z",
     "shell.execute_reply.started": "2022-10-02T12:16:21.788918Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# copy from Sidarthe model\n",
    "input_steps=1\n",
    "output_steps=2\n",
    "\n",
    "start_index = VAL_IDX+input_steps+output_steps\n",
    "x = last_update_pydatetime[start_index:]\n",
    "\n",
    "plt.plot(x, predictions[input_steps+output_steps:,0], label ='predict infected', color=confirmed_color )\n",
    "plt.plot(x, canada[start_index:][:,0], '-.', label ='actual infected', color=confirmed_color )\n",
    "plt.plot(x, predictions[input_steps+output_steps:,1], label ='predict recovered', color=healed_color)\n",
    "plt.plot(x, canada[start_index:][:,1], '-.', label ='actual recovered', color=healed_color)\n",
    "plt.plot(x, predictions[input_steps+output_steps:,2], label ='predict death', color=death_color)\n",
    "plt.plot(x, canada[start_index:][:,2], '-.', label ='actual death', color=death_color)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"date\")\n",
    "plt.ylabel(\"number of infected\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=10)\n",
    "tikzplotlib.save(\"tft.tikz\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ecbcb59d-40c2-406c-98a8-2d1deb074935",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T19:50:28.508513Z",
     "iopub.status.busy": "2022-10-02T19:50:28.508019Z",
     "iopub.status.idle": "2022-10-02T19:50:28.519340Z",
     "shell.execute_reply": "2022-10-02T19:50:28.516297Z",
     "shell.execute_reply.started": "2022-10-02T19:50:28.508471Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46690140.86441061"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(predictions[3:,0], canada[VAL_IDX:][3:,0], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7190a25d-e938-4ab0-b8a8-25ed5b1cb9e9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T19:50:29.261475Z",
     "iopub.status.busy": "2022-10-02T19:50:29.261009Z",
     "iopub.status.idle": "2022-10-02T19:50:29.271941Z",
     "shell.execute_reply": "2022-10-02T19:50:29.270031Z",
     "shell.execute_reply.started": "2022-10-02T19:50:29.261430Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41675153.81865376"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(predictions[3:,1], canada[VAL_IDX:][3:,1], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "c0702d34-2df4-412c-9494-e6b3e38d8caa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T19:48:53.371360Z",
     "iopub.status.busy": "2022-10-02T19:48:53.369517Z",
     "iopub.status.idle": "2022-10-02T19:48:53.407365Z",
     "shell.execute_reply": "2022-10-02T19:48:53.405876Z",
     "shell.execute_reply.started": "2022-10-02T19:48:53.371244Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2958386.1495794775"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(predictions[3:,2], canada[VAL_IDX:][3:,2], squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a43e45-e508-4578-84a6-8ee7b30b3875",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T14:23:57.930434Z",
     "iopub.status.busy": "2022-09-24T14:23:57.930120Z",
     "iopub.status.idle": "2022-09-24T14:23:57.940456Z",
     "shell.execute_reply": "2022-09-24T14:23:57.937208Z",
     "shell.execute_reply.started": "2022-09-24T14:23:57.930389Z"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3c73fd-5fd7-41b1-aa50-55a012ed3e6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-24T14:26:04.779507Z",
     "iopub.status.busy": "2022-09-24T14:26:04.779188Z",
     "iopub.status.idle": "2022-09-24T14:26:04.792199Z",
     "shell.execute_reply": "2022-09-24T14:26:04.790738Z",
     "shell.execute_reply.started": "2022-09-24T14:26:04.779471Z"
    },
    "tags": []
   },
   "source": [
    "### ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2c9922e4-5d68-4058-bda3-266c42cb5e8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-01T23:35:05.751680Z",
     "iopub.status.busy": "2022-10-01T23:35:05.751240Z",
     "iopub.status.idle": "2022-10-01T23:35:05.758462Z",
     "shell.execute_reply": "2022-10-01T23:35:05.757564Z",
     "shell.execute_reply.started": "2022-10-01T23:35:05.751639Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "def arima_prediction(column):\n",
    "    model = ARIMA(canada[:TRAIN_IDX, column], order=(32,0,0))\n",
    "    model_fit = model.fit(method='burg')\n",
    "    predictions = []\n",
    "    for t in range(VAL_IDX, len(canada)):\n",
    "        output = model_fit.forecast()\n",
    "        yhat = output[0]\n",
    "        predictions.append(yhat)\n",
    "        model = ARIMA(canada[TRAIN_IDX:t,column], order=(32,0,0))\n",
    "        model_fit = model.fit(method='burg')\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f2075dd5-283d-4f26-bc78-8b7b6dc3efea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-01T23:35:07.565586Z",
     "iopub.status.busy": "2022-10-01T23:35:07.565171Z",
     "iopub.status.idle": "2022-10-01T23:36:07.934386Z",
     "shell.execute_reply": "2022-10-01T23:36:07.933692Z",
     "shell.execute_reply.started": "2022-10-01T23:35:07.565539Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "infected = arima_prediction(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "85a7b14d-42f7-4578-847d-e686e2ac0705",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-01T23:36:07.936169Z",
     "iopub.status.busy": "2022-10-01T23:36:07.935946Z",
     "iopub.status.idle": "2022-10-01T23:37:06.760704Z",
     "shell.execute_reply": "2022-10-01T23:37:06.759989Z",
     "shell.execute_reply.started": "2022-10-01T23:36:07.936137Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "healed = arima_prediction(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2c0944f6-6a63-4d11-9cea-d5fbb7f6cf6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-01T23:37:06.763023Z",
     "iopub.status.busy": "2022-10-01T23:37:06.762799Z",
     "iopub.status.idle": "2022-10-01T23:38:04.674044Z",
     "shell.execute_reply": "2022-10-01T23:38:04.673231Z",
     "shell.execute_reply.started": "2022-10-01T23:37:06.762998Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "death = arima_prediction(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "002f3225-7ce5-4c73-b646-75434829fde0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-01T23:38:04.675882Z",
     "iopub.status.busy": "2022-10-01T23:38:04.675533Z",
     "iopub.status.idle": "2022-10-01T23:38:04.680164Z",
     "shell.execute_reply": "2022-10-01T23:38:04.679235Z",
     "shell.execute_reply.started": "2022-10-01T23:38:04.675847Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = np.stack([infected, healed, death], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "847d7206-e36a-4081-a21b-196260bb6821",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-01T23:48:16.549058Z",
     "iopub.status.busy": "2022-10-01T23:48:16.548750Z",
     "iopub.status.idle": "2022-10-01T23:48:16.553502Z",
     "shell.execute_reply": "2022-10-01T23:48:16.552473Z",
     "shell.execute_reply.started": "2022-10-01T23:48:16.549025Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inversed_predict_arima = mm_scaler.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "57ec055d-9519-4b18-bf36-815d15fc0e30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-01T23:38:04.682337Z",
     "iopub.status.busy": "2022-10-01T23:38:04.681945Z",
     "iopub.status.idle": "2022-10-01T23:38:04.827726Z",
     "shell.execute_reply": "2022-10-01T23:38:04.826477Z",
     "shell.execute_reply.started": "2022-10-01T23:38:04.682289Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tikzplotlib\n",
    "\n",
    "confirmed_color=(217/256,95/256,2/256)\n",
    "healed_color=(27/256,158/256,119/256)\n",
    "death_color=(117/256,112/236,179/256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1d27b447-326a-4ae2-b1bb-5a0bebfd387b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T00:46:00.836552Z",
     "iopub.status.busy": "2022-10-02T00:46:00.834676Z",
     "iopub.status.idle": "2022-10-02T00:46:00.879309Z",
     "shell.execute_reply": "2022-10-02T00:46:00.878324Z",
     "shell.execute_reply.started": "2022-10-02T00:46:00.836390Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "canada_csv = pd.read_csv('canada.csv')\n",
    "date_series = pd.to_datetime(canada_csv['Last_Update'])\n",
    "last_update_pydatetime = [datetime(year=x.year, month=x.month, day=x.day, hour=x.hour, minute=x.minute, second=x.second)\n",
    "                          for x in date_series]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a0bb10a3-b46e-4439-a21d-4f9a3d5a4880",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T00:56:31.561245Z",
     "iopub.status.busy": "2022-10-02T00:56:31.560869Z",
     "iopub.status.idle": "2022-10-02T00:56:32.127337Z",
     "shell.execute_reply": "2022-10-02T00:56:32.126314Z",
     "shell.execute_reply.started": "2022-10-02T00:56:31.561197Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAELCAYAAADURYGZAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABPfklEQVR4nO3deXxU5b348c+smZnse4AEEtYAQiB5UBDBJaL2CrjS2rrbYmvrbXu9Valtb5ff7S3WXr1qtZZWpdxqbV2wansBETDKIh4QZJEtJBAC2Rcy2WY7vz/OJIRAQoBMkiHf9+s1r0xmzvKdCZzvOc/znO9j0nUdIYQQg5e5vwMQQgjRvyQRCCHEICeJQAghBjlJBEIIMchJIhBCiEEuHBOB3luPsrKyXttWXzwGarwDNa5wjjccYgy3WMMlzhDG2qVwTAS9xu/393cIZ2WgxjtQ4+pKOMQbDjG2CZdYwyVO6PtYB3UiEEIIIYlACCEGPUkEQggxyEkiEEKIQU4SgRBCDHKSCIQQYpCTRCCEEIOctb8DEEKIcKYHAhDwo+sBTFY7JpOJQHMDWGyY7Q4CrY0EmurRA/725Qj40YMPk8mEPXU0AZ+H1uIt2JJHojcdp2lfEf76cvSAD93vBR0iJ12DNSal1z+DJAIhRK/RdR3d24rubWl/BFrdBJqPE2hpwN/aiGN4Drq3lcYdK7EljcASmUBLyXY8x/ah+1o7PLzoPg+6z4PFFYtz9Az87moad72Pa+xsTHYHTXsK8LurQQ+AHjAOtrpu/B7wY0vKxJY2Bm/1YVqO7OboxCvR/T6adr4P6KDrxoFZ19HRCehg0QOY0sbhiUrEXrYXfB7q07Jp9DTTVHUYr9mC12TGbzbjx4zPZCHO20isr5naxExqbS6GVR3EY3eyP2YYfr8Xv8+LbjITMEHAZMaPiYDJTFZjFZH+Vg5GJmPSdTKaa6izudgflWIsj7F8wGTCpOss2LeB2fc+3+t/N0kEQggAAs0N+BtrsCYOx1d3lIbNr+M/XoG/qY5AU53xs9mN3uom4GnEMWo6ZouNxh3vc9Bmxxo3BG/NEXzVh7rfD+AzWbDrfppNVgqjUqi1u6i3OWm0Omiy2Gm22Gm22mkx27jIXYW5sYlP9B1E+z0MscRSUlXFvohYfIk5BJLA3+Hg6jeZMOs6IzwN+C12DpqiSI6PwR49hipbFI02M7pa2F5zob32gsmEVQ8QoQfwmix4TCbMyXlGfQaTyVgms4df5ojZZ/v194hjxEWEYsuSCIS4wHkqi/BVHcJbX4a38iDe8kK81Yfw15fjb6zGnjIasyuW1sPb8dWXYzKb0X2eUzdktmCy2DBZ7JisdmorijgaO5SExEy8Mcm8Hj+KmnSo9/txmy00YqYJEx7Ao8NIpwuHxUZhSyNNfh92swW3z9tt7CZAS5+K02qnydtKpC2Co84oWv0+WlqasJrN2ExmnGYzNrMFq9mCzWwmwmJjWmomZiC+soRx8anYvAGOm/zUtTZjNZuxms1YzGZsJkvwdwvxEU5GxCTh9fs41nic0XHJ2C1WypvqMWHGZjZjs1ixmszYzRasFgtWk5koWwSRtgi8gQA6OrF2ByZMeAM+LCYLZrMZCyYswf2ag89NgMlkwmwynptNJswmE0ePHiN92DDMJlP76yZMmNoSUi+TRCBEmNMDfrzVhwm0NGBPG0fT7jVUvfljXBddS8BdxfFPXkNvbTxlPVNEFGZnDDpgjR2CbdpoLI5YbMPG0+hKoNoeRaHPx86GWvYfr+Gwu4bK5gYSHVHE2B0UH6+mtrUJIjONDfqC28WKDQsRFhtOq414qx2XzU5mTCLpUQlM8fuwW6yMTxyCy2rH4/OR5Ioi2RFFXISLSFsEdotxUO9NpaWlDBs2rFe3GSoOi5UIS98dniURCBEmfMcraT3yOd6y/bQe20Pr4e14yg/gP14OAT8mawR6wAcBP5jMBDxN2FNHE513I9bkUdhTR2NLzsSWMBxvVBLHWtyMjEmixF3Ldz9+g6nJGbT6fbxXtIOyps9O2b/NbCE+wkWiI5LclOHMzZxEtN1BtEdnbHomMXYHsXYnTqstZGeuIjQkEQgxgOi6jr++DN1vnF6Xv/oQ1tg0bPHDqPvoZZq/WAtmi3GwBzBbsSZkYB86HmeWImLEVCKGTsCWMhKT2UJNSyMH6irZV1fO1srDfLF7K4cbVlPvacZsMhFvd1EdvFrYU1PGmLgUJielc5UjiqzYRLLjUhkRk0iaKxaXzX7amEtLSxkWn9on348IDUkEQvQTf/NxPEd20npkBy2HttNS/KkxcqbVjTlzOgebq/CWHwDAZHdiSx1L9LQvEzFiChHDxmMfNhFbUiamDk0oH5TsoaK+jhF6MW8e2Mpf9285Zb/xES4mJw5jVGwyKmUEFyUNZVx8GlG2iD777GJgkUQgRB/wu6tpLd2Na9wsWkt3U7L4KvwNlcabJrMx3BHAYseWNo5AbBqx024gYtgE7EMnBA/4Zlr9PgrrKviitpxtB3ex7ZMVHKyvIidpGM0+L59Xl9IavJoYGhnD9LQspiSlMzkpndFxKYyMScJhtfXTtyAGKkkEQvQiXdfxVR2i5dBWWg59hi1xOPYh2dSteYGGT9/AEpuKv7YUTGZsKaOIGDEVR6YiYtgEIoZNxJo4HJPZTGlpKY6UJJxWG2aTme99+Fc+rTjEEXctAV3HxIlhj1HWCJp8HqYmD2fBmDxGRCcwLj6NJGdUf34VIoxIIhDiPPhqj9K07yNairfSWryVlmKNQPNxMJmNzltfC+g6loR0oqbMxTn2MhyjLsYxIhez3dm+Hbe3lU8qS6g7tIuZQ0exvGQni1eu5fKhYzhQX0lpYx0ALqud3OQMZg0bw9TkDLLj00hwRPbTpxcXCkkEQpyl2tXP4xg5DVtyFmV/+jaNn/8Tc0QkAU8L+D1gd+LMmoZz9AwcIy/GOeoSrHFD2tfXdZ2i41VsObSbTWVFbCo7yOGGGnQgwmJtb9pJcUUTZXNw34RLmZQ0jNGxKSQ7o2REjuh1/Z4IlFKjgDsBN/C0pmnd32EiRB/xu6tpPrDReOxbT8L8H+E9tpfqfywGvx9/QwUAtmETcI2ajmPUJThHXoJ92ISTOnAbva1UuOsYGhXHuwc/5+H1b+L2tmLChB5s4EmIcDE1OYOL07LISUonyWMmO3Nkv3xuMfiELBEopWKB94EJwHRN03YGX38cuBQoBu4DFgJHABmyIPqV73gFzXs+pPGLtTR9sRZv2T7AhMnuRPe2UPqb67DEpOIcdQmOUZfgGHkxjiyFxRlz0nYqmhrQKg6RHZ9Ks8/Lo+vforK5AZctgv11FZiAcXGp5GeMY1pqJjlJGaS4ok/aRmlpad99cDHohfKKoAm4Hnii7QWlVA4wTNO0WUqpHwG3Ag5gJZALzAY+CGFMYpDTdR3d0wxAa+ku6tf/mdiZd9B6ZCfHXrgT0DHZXeieRjCbicichmvMjPazfWvi8PamGW/Az/76SnYfLWRX9VE+qyxhT20Zxz0tmACr2YI34CfaFsGkxGFckpbF1OThqNQRxNgd/fclCNFJyBJBsImnUinV8eVLgVXB5yuAe4E/Bn9GAv/vdNtSSt0P3A/w4IMPMmfOnF6J0ev1htWZ10CNdyDGpXuaCFQXE6g8QKDyIIFjuwhU7EOvK8UUn8GBhOH4j+2Chgpq//lrMFshJhVz6jgsadlY0qdgGTUTkyMGL+AFjjQ0896e97gsOYtjLQ38v53vU9XaRFuLvQ6YMDHcFcfU+GFMSRjKRbGpZLjiTmrXb6ispuEM8Q/E77Qr4RJruMQJoYm1u/Iafd1HEA8cCz6vBxI0TdsGbOtuJU3TlgBLgr/q3S17NsKp9ggM3Hj7I66AtxVf3VHsyVnouk7V8p8SOeFqzI4oKt/4EU072843TGAytY/TNzmiwe4kKm0k9inXYR8yFnvaOGzJIzFZbTR6W/mipozdNcfYcWw3WysO4w34mTV0NJ9Xl7K98gjP7FsPQLIzKjhOP4MJiUPIjktlZGxyr4zTH6h/69MJl1jDJU7o+1j7OhHUAW0NqrFATR/vX4QBXdcxmUztnbW+2lK8lUV4ju3FU1GIr7aUQFM9AM6J+fhrS/Ec20fNO780NmCxY03KxD5kHBHDpxAxNNs42KeOxRIZx9GjRym2tGI1WUhwuPigZA/LN6+ixF1rFFEDLCYzAT2ADkTaIjhQX8nU5AxuGzuN7PhUxsalEi3NO+IC0deJYAPwELAMuBZY38f7F/0o0NqIr+4Yvvoy/HXH8NWX46s9gqeyiKip8zBHRFGz4r8JNB3HOepiWkt20FL0afCMvsOFoNWONTEDW2IGtqQsIrOvwJY4HBLSKY2Ipcrm4rC7lsLjVZQ01FBSWUv54fepbf07U5IyqG1yc6ixFl/b3byA1WQmNsLJRQlDGRmbSGZMEhclDmNKUjpDImNlyKa4oIU0ESil/glMAcYppX6vadpSpVS5Uuoj4DDwm7PY1jxg3pNPPsns2aGZ9EGcPV3X0ZtqaT1Si6/uGJaYFBzDc2j8Yi316/5I/JwH8daWUrbkHnRvc4c1T26ycX/6BpjMWCITMEcl4qsvx54xGftF19AQm0aNM55aZyxVVieHW5rJiE4gLTKGfxbv4JOyYi7x2Thc+AWby4tPidFuthJrdzA2LpXM6ETyYoYwbkg6aa5YhkTGkuaKkbN7MaiZdL3Xmtz7ivQR9AE94MdkthDwtNC0632cYy/DEhlP9T9+TdPu1fhqSvE1VBJoqjtRCRMwu+JADxh317a/aMHsiMESk4Q1bijWxBF4kjKpi0xCj05m/JDRVEfE8PviXczNyqHF7+XxLSvZX1dB0+kmSAkyYSLe4cJltXNR4lDSXDE4LDZGxSaTGZPIkMhYUl0xODu12YfD3z0cYmwTLrGGS5wQsli7vKzt9xvKRN9qO8B7K4tpLtqMv64MX90xvLVH8FUdxld3FH9DJSabg9jZ9+GtOkTDJ69hTR6J3lxvzA/bxmLDEp1MwJWAKzULW2Im1sQMih1xJMalMSQ5k/XNzfxf+WFKG+soa2qgusXNcU8L3tpmqC3BaSkj5tABKpsaCKDz0hcbAeMO2xi7g+HR8aQ6Y0mPjiMrJon0qHiGRMYyxBVDsisaWy9PXiLEYCSJ4AKh+zz4ao8aB/TaUuOMva4UX80Rkr7ya/C2UPrsrViTRhAxbCJNu9fQevgzMFnaJ/5uZ7Zgtjlp+mIdlthUYq+4Hz1uCGWRyVQ5Y6mwuSjTLRxubaK0sY6jx2up97VyZ/Yl1LY28daBz4irqCKwbwcVTQ3td88COK12UpzRJDujGBIZy4joBDKiE0h2RpPijCbFZbzntJ6+9r0QovdJIggTus/bfiNU8/4NtB7dTdzl38BbWcyhX87CX3/sRIeq2YLJ5jTKG/s9NHz6Rvt2fMcrCDTVYUsdhXPcbGxxQ/DEpFDljKPCFkml2YZKzybJGc0z29dQVF/FFenj2FJxiDcLjVmrOpZGaBNptbO65AuGRMZx9fDxDI+KZ2hUHKmuGFKc0aS5YkhyRmHvw+n3hBA9Ezb/KwdDZ7Gu6/jrjtF67As8R/fiObYHT9lePGX78NUcwTr9buonXUHD5jdoLdlOw6bX8FQUnpQETK44bMkjsSdnYUvKxJachTlpBDVRqYzImIDZ7uSVvZs50lDD4YZatIpDlJbtbo/BjAnbllXthc8AtlcdIS0yhiuGjiErJokRsYkMi4wjLdgG762pZ0TG8D7/voQQvSNsEoGmae8C72LUJrog+BvrqP/oZWIvuxtLVAKH/3MmLYWfAGCOTMASGQ8mE3qww9S38WXKt/4NW3IWERmTsSVnETllnjEPbfCgb3HG8EHJHtaU7GFIZBw3j5rC3/Zv4Tcfvs4NWZM50ljH51WleDt08AJE2xxckpZJbvJwhkbFMSwyjmFRcaS5Ys54Fl9a5w7NFySE6BNhkwjCma+hipaDm9sfus9LxqPv46svo+a9xbQc3Ezr0d14juxsXyfQWIPFFUtE+mTjpqiMydRFpJE+8ZL2Me3VzW60ysOkuWJo9nk5UlrI+mMH+Nv+LUxLzeRIYx1PfvY+vkCAoZFxNHhbUSkjuHHkFFJc0aQ6Y0h2RpHiipY2eSEGMUkEIeCtLsG99W2aCz+h5eBmvBWFYDJhSx2LNS4N9ABFP5yI59gesNjw1ZYSNXU+rtuewOyKB78X+7CJWFyx6LpOeXMDFrMFc3Udz2xfw6GGGu6bcCnvHPyc53d8eNK+7WYLLquNT8uLsZkt/FBdxz3jZ0jbvBCiS3J06CXHN/8N//Eq4q/+Np6yvVT/43EcmXk4shT2tHG0HtuDt2wvvqpiHKOmE33xApzjZuMcNR1zhOvEdjwtbK8sYdv+rWyrLOGzyhIqmhu4b/ylOHw6m+tLOVBXyd/2byEhwsUlqZkMi4pjTGwqaa5oypvdJDkjGR6dwJi4FJKd0d1ELYQQYZQIBlJnsb+xjqbdH9C4YyVRU+cRNXUe/uOV+OqOous62BxETv4X3J++TsDTjGvsLGIvuwvXuNk4Rl6C+TR3sf5ux4e8eeAz9tSWAZDqjCYmwonDYsVqMvPSFxsY6ozh4iEj+dKIi7g8fSzj4lKl9IEQ4ryFTSLo787i1pIduD97l8adK2k+sBFMJpyjZxplEoCoKXOpX/+/FC3Kxlt+AEfWNJJu+SUx07+CJSqxfTttd3J7A34eWPsq/5pzJWPjUtleeQRfwE9mdCJVLW4qmt0kOaOYmTGKS1KzUKkjCNS5w+bOSCFE+AibRNBfGrS3qH73V7Qe2ootKRPXpGuJv+4hXOOvxGQy06C9ScnjV9P0xVqscUOJufQOYi67i4ih49u34Q342XjsICsO7aLg6H7ev/F72MwWvH4//731fT4pK6LV72PWsDFMThrGEFcs1wyfcOqsVTI6RwgRApIITsO97T1MdheRE64i0HwcR1YeqXc/hyNrGgDNewuo+PN3adDegoCfqLybSP/B/+GakN8+V60v4Ofjo4X8/eA2Vh3eTb2nhanJGVw+dAw/++Q9Vh7aTVWLm0vSsvjJxddzfeZFxDsi+/NjCyEGKUkEQb7jlZjMZixRiTRsfh1b4ggiJ1xF7Kx7iJ11DwCNO1dR9dbPaDn4Cc6xl5HytSeJnrYAiyu2fTv+QIBfbP4H7xRtp6q5kUvSMrln/KW4va2sLvmCP+3ZxISEIXxz0ixuyMphaFRc/3xgIYQIGtSJIFB5gOptr9D4+f/RfGAjifMeI+nG/yBt4dKTOmFbirdQ+foPadr1AVHqZkbcsxXH8Jz29+tam3hx13oWXjSLGLsDt7eVO7On4/P7WVO6l6e3r2F4VAI3jsrhhpFTGBef2h8fVwghTitsEkFvjRryHa+gvuBljn/8J6N8Q9xQInP+hfirHyRyylyA9iQQaG6g8o3HqPvgeZzjLmf4f2zEOfJiwLiZa23pPm4dnYvNbOEfxTu4ZvgEkpxRxNodvLCjAIfVxvysyfxyxg3kJg+XET5CiAEpbBJBb4waql3zOype+TcsrjhiLrsbS+ZvyJh2HSaz+ZRlG3espGzpt9C9rQz9zl+JUrcAsKOqlL8d2MJf9n6K02ojP30c8Y5I/ph/Fy/sKOCNA1tIcUXzmPoSXxmrTqmFL4QQA03YJILe4BwzkyELXyYq72bMtghKS0tPSQJ+dw0Vf/l3jq9fRszMO0n56pMc8vv5w7YP+PvB7RTWVzImLoWfXnI9Xx6jqG1t4l8/fI2/H9zO6Nhkfn3ZLdwwMkfq5AshwsagSgSOjMk4MiZ3+b5723uUvfxNTBYbwx56j6jJX+KrK/7IR0cPMCwyjhtG5nDjyBzGJwzhUEMNi7UVvLp3M2mRsSy56g6uGT4es+nUqwshhBjIBlUi6Iqu61T//f9R/fbP8V7+TX6RMomfDcthArBgTB7/NuVqVOpwzCYzn1WW8PUPlvH+4T2MiEngUXUtt4+7hAip5SOECFOD/uil6zqVr/2Ayg9+x9BvvETczLu4+LPVWINNOzePmgrAntoyHtdW8n7JF0xPy2LpnLu5Mn2sXAEIIcLeoEwEuq6zv66Cj4/uxb3hRUr2fcZnVy3itvhRfMNk4ge5c9qXPeKu5Tdb3+fNA58xJTmdv35pITOHjOrH6IUQoneFTSLojeGj/kCANw5sZcmuj9hbW06EyUxccy1Joy5jyvAJzMk4URZC13Ve2FnAE1tWMSImkT/k38G1wyfIEFAhxAUnbBJBbwwf/c3W9/n9zgJuGzuN/5l5C9bHryJm+CSG3v78SQd4Xdf5lbaCF3YW8NOL53L3+OntTUVCCHGhCZtE0BvumTCD28YqRsQkUr9+GWV1paT8aN1JSSCgB/jJxnd4dd+nPH/F15ibNakfIxZCiNAbVIkg1RUDgB7wU/3ur7DmLsCWkN7+/uayYn6++T321JbxYv5dXJUxrr9CFUKIPjOoEkGbxu3/xFt+ANftLwHGrGCL1r/FO0WfMydjPM9e/hVGxib3c5RCCNE3BmUiaNj6Ns6xszAnDKe62c0dq16musXNq9d+ndnDxvR3eEII0ae6TARKqZe6WU/XNO3rIYgn5PSAn8Zt75Ew9zEadZ3vrHsNt7eVt69/QEpCCyEGpe6uCO7p8FwHTJ1+D8tE0Lx/A/6GKqJy57Ns/+d8Ul7EP+Y/KElACDFodXdb7LTg47+Bj4CrgWuAAuC3oQ/tZEqpeUqpJQUFBee1Hfdn7xKRMZnmmDSe37+Rf5uSz4SEIb0UpRBChJ8urwg0TdsCoJT6B/BzTdPWBH8fC/wQ+F6fRHginl6ZvL6leAvOcbP5895PsJotfGPiZb0ToBBChKmedBY3A79SSk3HaB6aD1SHNKoQ8lYUEpF7Ay/tWs8tGZNw2ez9HZIQQvSrnlRM+wZGMrgTuANo4jzPyvtLwNuKr/YI622x1LQ2sWB41yWphRBisDhjItA07QNgBDAl+MhsayYKN97KItB1Nnj9qJQRJEa4+jskIYTod2dMBEopE8YVwC8AG/CQUuqqUAcWCt6KQnSTiY9rK7hc7hcQQgigZ01D/wU8C8wDYoHxwM9DGVSoeCsPUpY6niONdXLjmBBCBPUkEdwF/L7D7x8DYVmJzVNRyNa0icRHuLgocWh/hyOEEANCTxKBEzjW4fdhgDc04YSWt6KQbdFDmDlklMwsJkQYW7duHXPnzgXgnXfeYfHixV0uW1dXx/PPP9/l+5deeukZ9/fRRx8xceJEpkyZQnNz81nF+vbbb7N79+6zWgcgKirqrNc5Vz05Gq4FHgo+/w3wIyA8O4srDlJqdTI6LqW/QxFCnIbf7z/rdebPn8+iRYu6fP9MiWDDhg1n3Mcrr7zCD3/4Q7Zt24bT6Tyr+M41EfSlniSCfwU+Cz6fgnGX8fdDFE/I6IEAnsoiSnUYEZ3Q3+EIMagUFxeTnZ3N7bffzvjx47n11ltpamoCIDMzk0cffZTc3Fxef/11Vq1axYwZM8jNzWXBggW43W4AVqxYQXZ2Nrm5ubz11lvt2166dCkPPvggAOXl5dx0003k5OSQk5PDhg0bWLRoEYWFhVxzzTU8/PDDp8TWdua9bt06rrjiCm699db2WHVd549//CN/+9vf+MlPfsLtt98OwBNPPMG0adOYPHkyP/3pT9u3tWzZMiZPnkxOTg533nknGzZs4J133uHhhx9mypQpFBYWUlhYyHXXXUdeXh6zZs1iz549ABQVFTFjxgwmTZrEr3/96xD8FbrWkxvKRgNfxrh/AMAP9PmR9HynqvQ31tCYkIE7ECAjOr73AxQiTOg+D96q4l7bXqCqHI+lEVtSJiZr1zdo7t27lxdffJGZM2dy33338fzzz/ODH/wAgMTERLZu3UpVVRU333wzq1evJjIykscff5wnn3ySRx55hIULF7JmzRpGjx7NV77yldPu47vf/S6XX345y5cvx+/343a7Wbx4MTt37mTlypUMGzas28/y2WefsWvXLoYOHcrMmTNZv3493/jGN/j444+ZO3cut956K6tWrWL//v1s3rwZXdeZP38+BQUFJCYm8p//+Z9s2LCBpKQkampqSEhIYP78+e3rAuTn5/PCCy8wZswYPvnkE7797W+zZs0avve97/HAAw9w11138ctf/vIc/xrnpieJYC1wm6ZprwMopb4CvAr06dyN51tiwhqdhOWRD+Cd3zI8Sq4IxODlrSqmaNH4My94FoqArMVfYE8b2+UyGRkZzJw5E4A77riDZ555pj0RtB3YN23axO7du9uX83g8zJgxgz179pCVlcWYMWPa11+yZMkp+1izZg3Lli0DwGKxEBsbS21tbY8/x8UXX0x6ujFZ1ZQpUyguLuayy04uQ7Nq1SpWrVrF1KlTAXC73ezfv5/t27ezYMECkpKSAEhIOPU443a72bBhAwsWLGh/rbW1FYD169fz5ptvAnDLLbfwq1/9qsdxn6/uylDPBq7AKCuxQCnV9i9nNmHaWVzSUIPNbGmfqUyIwciWlEnW4i96bXvl5eWkpqZiS8rsdrmOU8J2/j0yMhIw5gufM2cOf/nLX05adtu2bb0S65lERES0P7dYLPh8vlOW0XWdH/7wh3zzm9886fVnn332jNsPBALExcV1+Xk6f0d9pbs+giuBn2GUnL41+PxnwFUYFUjDzuGGWtKj4rGYZcSQGLxMVjv2tLG99jAnjcSeNrbbZiGAw4cPs3HjRgBeffXVU860AaZPn8769es5cOAAAI2Njezbt4/s7GyKi4spLCwEOCVRtMnPz+d3v/sdYHQ819fXEx0dTUNDwzl/X51de+21vPTSS+19F6WlpVRUVHDVVVfx+uuvU11tlGKrqakBOGn/MTExZGVl8frrrwNGUtm+fTsAM2fO5LXXXgNg+fLlvRZvT3R3RPwbRt+ACfgfYAFGQsgHrg95ZCFQ4q5huPQPCNEvxo0bx3PPPcf48eOpra3lgQceOGWZ5ORkli5dyle/+lUmT57c3izkcDhYsmQJ119/Pbm5uaSknH7k39NPP83atWuZNGkSeXl57N69m8TERGbOnEl+fv5pO4vP1jXXXMPXvva19o7dW2+9lYaGBiZOnMiPfvQjLr/8cnJycnjoIWOw5W233cYTTzzB1KlTKSws5JVXXuHFF18kJyeHiRMn8ve//7099ueee45JkyZRVlZ23nGeDZOu690uEGwSOqJpWkPw90jAr2laSx/EdzrdB9yN21e+REZUPItn3gQYmfxMnUcDyUCNd6DG1ZVwiDccYmzTk1iLi4uZO3cuO3fu7KOoTnWhfafnoMt2p560kfwNo85Qm/8EPj3fiPrD4YYaGTEkhBCd9CQRjAY+7/D7DmBUaMIJnYAeoNRdK/cQCNEPMjMz+/VqQHSvJ8NHjwDfUEptxLi0WAiUhjSqEKhrbSbZGc1wSQRCCHGSniSCF4AngF3B303AD0IWUYgkOCL55Ctd34YuhBCD1RkTgaZp/62UqgTmBl96V9O0/w1tWEIIIfpKT64IAFYCx4HNgFUpFd02ikgIIUR468kMZVcDB4A3gOzgzxdCHJcQYhBbt25dj6qCdqerMs5SdvpUPRk19BtgNyfGoL4OXB6yiIQQg15vJIKuSNnpU/V0+OhbHX6vBeJCEo0Q4oJ14403kpeXx8SJE08qGLdixQpyc3PJyckhPz+f4uJiXnjhBZ566immTJnCRx99xD333MMbb7zRvk7b2bPb7SY/P5/c3FwmTZrUfpdudwZi2ekf//jH5/8Fn4ee9BEcAG4IPp8D3AzsDVlEQoiQ87ur8buru3zfZLFjS85E9/vwVh7EmjAcs92Bt+YIuqfppGUDVeX4Yx1YohK73edLL71EQkICzc3NTJs2jVtuuYVAIMDChQspKCggKyurvXTzt771LaKiotqrk7744oun3abD4WD58uXExMRQVVXF9OnTmT9/fo+Ltw2UstPPPfdcj+INlZ4kgh9j9AuYgEcBD3BTKIM6nfOdj0AIcULt+7+l+u+/6PJ9e/oksv5zG/7jFRQtGs+In23GkZlH2UsLadq56tTt3fAfJN3009Ns6YRnnnmmvZhaSUkJ+/fvp7KyktmzZ5OVlQWcvnRzd3Rd57HHHqOgoACz2UxpaSnl5eWkpaX1aP2BUnb6zjvv5NFHHz2rz96buitDPRIo0zTtPaXUJIyrAYD3NU3b3yfRdXC+8xEIIU6In/MgMTO+2uX7JotRSdQSk0LW4i+wJgwHIO2+P5xyRVBeXk78qAnd7m/dunWsXr2ajRs34nK5uOKKK2hp6Xm5MqvVSiAQAIxSzh6PBzDa8isrK9myZQs2m43MzMyz2u5gLTvdWXd9BPuBuUopPzBF07Tng48+TwJCiN5liUrstrS0LTkTAJPFapSatjsAsCWkn7YM9Zmaherr64mPj8flcrFnzx42bdoEGGWnCwoKKCoqAk5fuhmMEhVbtmwBjMnqvV5v+3ZTUlKw2WysXbuWQ4cO9d6XFNQXZadfeeWVXo/7bHSXCFqB+zgxMc1/dHj8pG/CE0JcCK677jp8Ph/jx49n0aJFTJ8+HTDKTi9ZsoSbb76ZnJyc9pnK5s2bx/Lly9s7ixcuXMiHH35ITk4OGzdubJ/I5vbbb0fTNCZNmsSyZcvIzs7u9dj7oux0aWn/Vu3psgy1UmoFcA1G2efO1y+6pml9OlVlx3331obCqSwtDNx4B2pcXQmHeMMhxjbhEmu4xAl9X4a6u87imzFmKXsXY2Ka9b0bkxBCiIGgy0SgaVoT8A+lVBZQoWna2d1eJ4QQIiz0ZPhoOvBHpVQm0NYcpGuaFnZzEgghhDhVTxLBXzCSQStw6tgqIYQQYa2n1Ud/rGnaf4U0EiGEEP2iJ4ngbeBflFKfYNQZAkDTtK2hCkoIIUTf6UnRuQeBGcAqjEnr2x5CCNEv1q1bx9y5xlxZ77zzDosXL+5y2bq6Op5//vm+Cq1P9Vbp6p5cESyjF8fuCyFEV/x+PxbL2d2iNH/+fObPn9/l+22J4IYbbuhyGTDu+tV1HbO5J+fHoXO6MhehdsZPrGnaPZqm3dv50RfBCSEuDMXFxe2lnsePH8+tt95KU5NRsygzM5NHH32U3NxcXn/9dVatWsWMGTPIzc1lwYIF7aUdVqxYQXZ2Nrm5ubz11onK+EuXLuXBBx8EjLpHN910Ezk5OeTk5LBhwwYWLVpEYWEh11xzDQ8//PApcY0bN4677rqLiy66iJKSkh6XnG5b/6qrrmLy5Mnk5+dz+PBh6uvrGTFiRHttpMbGRjIyMvB6vV2WpL7nnnv41re+xSWXXMIjjzxCcXFxn5au7q7o3DvA4xgVRzvTNU3rPr0KIQYkj9/HEXftmRfsofLGWlrr7aRHxWO3dN3IsHfvXl588UVmzpzJfffdx/PPP99eZjoxMZGtW7dSVVXFzTffzOrVq4mMjOTxxx/nySef5JFHHmHhwoWsWbOG0aNHt5ei6Oy73/0ul19+OcuXL8fv9+N2u1m8eDE7d+5k5cqVp71bd//+/fzpT39i+vTpZ1VyGuBf//Vfufvuu7n77rt56aWX+O53v8vbb7/NlClT+PDDD7nyyit57733uPbaa7HZbNx///2nLUkNcOTIETZs2IDFYuGyyy7j5Zdf7rPS1d01Dc0FXuHEpPUdSVOREGHqiLuW2W/+d69vt+CWf2dkbHKX72dkZDBz5kwA7rjjDp555pn2RNB2YN+0aRO7d+9uX87j8TBjxgz27NlDVlYWY8aMaV+/4+Q2bdasWcOyZcsAo5pobGwstbXdJ70RI0a01z4625LTGzdubL86ufPOO3nkkUfaP89f//pXrrzySl577TW+/e1vd1uSGmDBggVYLBbcbjeapvVp6eruEkEWUBn8KYS4QKRHxVNwy7/32vbKy8tJTU0lPSq+2+U6l1zu+HtbETld15kzZw5/+ctfTlq2qzLOvaFt3237P9eS0x3Nnz+fxx57jJqaGrZs2cJVV11FY2NjtyWp2+IIBALExsb2aenqLvsINE07pGlaU/DnKY9ej0QI0SfsFisjY5N77TE8Mp6RscndNgsBHD58mI0bNwLw6quvnjIBDBhlqdevX8+BAwcAo3193759ZGdnU1xcTGFhIcApiaJNfn4+v/vd7wCj47m+vv6UktbdOduS05deeulJpaRnzZoFGKN5pk2bxve+9z3mzp2LxWLptiR1RzExMWRkZPRp6er+7R4XQgwa48aN47nnnmP8+PHU1tbywAMPnLJMcnIyS5cu5atf/SqTJ09ubxZyOBwsWbKE66+/ntzcXFJSUk67j6effpq1a9cyadIk8vLy2L17N4mJicycOZP8/PxTOos7O9uS088++ywvv/wykydP5n//9395+umn27f1la98hT//+c8n9Wd0VZK6s2effbZPS1d3WYZ6AJMy1APMQI2rK+EQbzjE2KYnsRYXFzN37lx27tzZR1Gd6kL7Ts/B2ZehVkr9AvgrkAd8KM1BQghxYequaehHwFTgZeDivglHCHEhyszM7NerAdG97np3qoHnMC4nfquUerzDe1KGWgghLhDdJYL/Ah4DooEYwBWKAJRS9wBjgQOapr0Uin0IIYToWnczlP0P8D9KqbXAzzVNW3c2G1ZKxQLvAxOA6Zqm7Qy+/jhwKVAM3AccB5oB59mHL4QQ4nz1pNbQlWAcwJVSi5VSs3u47SbgeuCNtheUUjnAME3TZgF7gFs1TXtL07T/B0QopUae9ScQQghxXs5YfVQp9Q3g95wYevSwUup+TdNe7G49TdO8QKVSquPLl2KUswZYAdyrlKrF6JQeBhzpIob7gfsBHnzwQebMmXOmsHvE6/X26ljcUBuo8Q7UuLoSDvGGQ4xtQhHrhg0bsNvtdDp+nJWxY8eyb9++9t8vhO+082c6G90NR+1JGepHgU1AWxm+nwVf6zYRdCEeOBZ8Xg8kaJq2AiMpdEnTtCVAW2ERuY9ggBmocXUlHOINhxjbhCLWXbt2ERUVdcbS0d0xmUwnxdU5zoFUdtpqPflQ3NV32vkz9ZaefANpwJ80TVutadpqjPkJhpzj/uowOp4BYoGac9yOECLM3HjjjeTl5TFx4sSTCsatWLGC3NxccnJyyM/Pp7i4mBdeeIGnnnqKKVOm8NFHH3HPPffwxhvtrcztE7K43W7y8/PJzc1l0qRJXd6p22aglp3uvFxbiY1QlZ3urCdXBLuBnyml0oO/fx041wHBG4CHMJLJtcD6c9yOEOI81LY0Utva1OX7NrOFjOgEfAE/hxtqGBoZh8Nq42hjPS0+z0nLljfW4mqJI94R2cXWDC+99BIJCQk0Nzczbdo0brnlFgKBAAsXLqSgoICsrCxqampISEjgW9/6FlFRUe3VSV988fQNEA6Hg+XLlxMTE0NVVRXTp09n/vz53RZmG4hlp/Pz809a7qGHHmL9+vUhKzvdWU8Swb8D7wBt6ag2+NoZKaX+CUwBximlfq9p2lKlVLlS6iPgMPCbngaqlJoHzHvyySeZPbun/dVCiNN5afcGntr2QZfvZ8ensfqm71PV0sjsN/+bf85/kMlJ6Tz88Zt8WHpqG/W/Tcnn33O777t75plnWL58OQAlJSXs37+fyspKZs+eTVaWUeS4rbxzT+m6zmOPPUZBQQFms5nS0lLKy8tJS0vrcp2BWHa683KNjY1A6MpOd3bGRKBp2sdKqdEY8xYDbNQ0rUdNOpqm/ctpXuu+6lPX23oXeBdYeC7rCyFOuG/Cpdw0akqX79vMxnSRSY5ICm75d4ZGxgHwxGW3nHpFUF5O9vDuq9WvW7eO1atXs3HjRlwuF1dccQUtLS09jtdqtbY3vQQCATweI4ZXXnmFyspKtmzZgs1mIzMz84zbHYhlpzsv17GjOBRlpzvrUS+Jpmk1mqb9I/iQdn0hwly8I7Lb0tIZ0cYZsNVsYWRsMg6rDYChkbGnLUN9pmah+vp64uPjcblc7Nmzh02bNgFG2emCggKKioqAE+WdO5eOzszMZMuWLYAxWb3X623fbkpKCjabjbVr13Lo0NmVRBsoZac7L7d7924gdGWnO5My1EKIkLvuuuvw+XyMHz+eRYsWtTfNJCcns2TJEm6++WZycnLaSzbPmzeP5cuXt3cWL1y4kA8//JCcnBw2btzYfjZ9++23o2kakyZNYtmyZWRnZ59VXAOl7HTn5VauXAmErux0Z1KGOkyG6MHAjXegxtWVcIg3HGJsEy6xhkuc0PdlqLu9IlBKWZRSbyil5vd2RGdLKTVPKbWkoKCgv0MRQogLSredxZqm+ZVS2cDwPoqnu1iks1gIIUKgJ8NHdwK/UEqN4MRdwWia9mTIohJCCNFnepIIvhz82fHeAR2QRCCEEBeAniSCe0MehRBCiH7TkxvK/qSUsmPMK1CkaVp96MM6ldxZLIQQoXHG+wiUUlOBQkADpimlvlBK/SHkkXWiadq7mqbdL0lACLFu3Trmzp0LGDeYLV68uMtl6+rqeP755/sqtD7VVnzvfPXkhrLfAo0YY1ADwJ+Bq3tl70II0YHf7z/rdebPn8+iRYu6fL+niUDX9fYyFv3J5/P1+T57kghygKUdfj8KpIQkGiHEBam4uJjs7Gxuv/12xo8fz6233kpTk1H9NDMzk0cffZTc3Fxef/11Vq1axYwZM8jNzWXBggXt5R9WrFhBdnY2ubm57YXfAJYuXcqDDz4IGHWPbrrpJnJycsjJyWHDhg0sWrSIwsJCrrnmGh5++OFT4hqIZamLi4tPu1zIylK3Tc7Q1SMvL29PXl7e/+Xl5fnz8vK+n5eX90leXt6uM60XwkevOXLkSG9uLuQGarwDNa6uhEO8oYzR6/XrFZUNvfbYsfOAXlHZoHu9/i73WVRUpAP6xx9/rOu6rt977736E088oeu6ro8YMUJ//PHHdV3X9crKSn3WrFm62+3WdV3XFy9erP/85z/Xm5ub9fT0dH3fvn16IBDQFyxYoF9//fW6ruv6yy+/rH/nO9/RdV3Xv/zlL+tPPfWUruu67vP59Lq6Or2oqEifOHHiab/ToqIi3WQy6Rs3btR1XddXrlypL1y4UA8EArrf79evv/56/cMPP9R37typjxkzRq+srNR1Xderq6t1Xdf1uXPn6kuXLtV1XddffPFF/YYbbtB1Xdfnz5+vr1mzRtd1XX/ttdf0r3/967qu6/pVV12l79u3T9d1Xd+0aZN+5ZVX6rqu63fffbd+/fXX6z6fT9d1XZ85c+Zpl5s3b57+pz/9Sdd1Xf/tb3+rR0ZGdv/HPlmXx9WejBr6NfDH4PO2IaP39F4qEkL0pdq6Jp54cm0vb3U3Dz90JclJXbdZZ2RkMHPmTADuuOMOnnnmmfb5Btrq82zatIndu3e3L+fxeJgxYwZ79uwhKyuLMWPGtK/fcXKbNmvWrGHZsmUAWCwWYmNjqa2t7TbygViWWtO00y4XqrLUPRk19JJSqhBjInqAf2ia9mGv7P0syKghIXpHfJyLhx+6ste2V15eTmpqKvFxrm6X61xOuePvbUXkdF1nzpw5/OUvfzlp2a5KOfeGgViWOjY2tsvlQlGWuqfVR2swJpI5TD9NLymjhoToHVarmeSkqF57xMc5SE6Kwmrt/nBy+PBhNm7cCMCrr77KZZdddsoy06dPZ/369e1TNTY2NrJv3z6ys7MpLi6msLAQ4JRE0SY/P5/f/e53gNHxXF9ff0pJ6+4MlLLUGRkZp10uVGWpezJ89N+BbcDTwDPANqXUv/VaBEKIQWHcuHE899xzjB8/ntraWh544IFTlklOTmbp0qV89atfZfLkye3NQg6HgyVLlnD99deTm5tLSsrpx6s8/fTTrF27lkmTJpGXl8fu3btJTExk5syZ5Ofnn9JZ3NlAKUv97LPPnna5UJWlPmMZaqVUFUaNoacwEsf3gRRN0/pr5JCUoR5gBmpcXQmHeMMhxjY9ibW4uJi5c+eyc+e5Tnd+/i607/QcdNmm1JPO4jLgGU3TXgJQSpmAU1O5EEKIsNRlIlBKPRR8qgH/oZQahnFFcC/Q9azXQgjRSWZmZr9eDYjudXdF8BuMZpi2y4n/6PDencgQUiGEuCB0lwgGVNVRGT4qhBCh0WUi0DTtT30ZyJnIDGVCCBEaZ+wsVkpdAywGsgBL8GVd07TYUAYmhBCib/TkhrKlwGSgAagOPvrlpjIhxOCwbt06NmzYcF7b6K0SzQNJqD5TT4aP+oDva5r225BEIIQQnaxbt46oqCguvfTSkO2jreCa2dzTAguh4fP5sFp7cigOnZ58A/cDDymlfqaUeij4kDuLhRBn5cYbbyQvL4+JEyeeVDBuxYoV5ObmkpOTQ35+PsXFxbzwwgs89dRTTJkyhY8++oh77rmHN954o32dtjNjt9tNfn4+ubm5TJo0qcs7ddsM1LLTnZdrK7ERsrLTnXVXmlQ3ylC/l5eXF+j08J9pvRA+ek04lCPuaKDGO1Dj6ko4xBvqGN2Nrd2Wlq6uadR1Xdd9PqNktcdjlEeurWs6bRlqd2PrGffZVrq5qalJnzhxol5VVaVXVFTo6enp+sGDB09a5qc//Wl7mWpdN8o0v/766+2/t5Vf9nq9en19va7rRgnrUaNG6YFA4KRl2hw5cmTAlp3uvNyll16q6/p5l53u7LzKUF8GrADeAryhS0lCiL6yfkMRq9fs6/L9tLRoHvruFbgbPTzx5Fq++51ZpA+L4423trNvf+Upy199lY9rrh7X7T6feeYZli9fDkBJSQn79++nsrKS2bNnk5WVBZwo79xTuq7z2GOPUVBQgNlsprS0lPLyctLS0rpcZyCWne68XGNjIxC6stOd9SQR/BnjxrKlmqb1/RxqQXIfgRC9Z+alWUyd0nUtG4vFaDWOirTz8ENXEhfrBODWm3Pwek+eTrK8vJysrIxu97du3TpWr17Nxo0bcblcXHHFFbS0tPQ4XqvV2t70EggE8Hg8gFHErbKyki1btmCz2cjMzDzjdgdi2enOy3UsKBeKstOd9aSP4G7g20CzUup48FEf4rhOIWWoheg9kS57t6WlE+KNuQUsFqNktc1mjByPi3Wetgx1pMve7f7q6+uJj4/H5XKxZ88eNm3aBBhlpwsKCigqKgJOlHfuXDo6MzOTLVu2AMZk9V6vt327KSkp2Gw21q5dy6FDh87qexgoZac7L7d7924gdGWnO+tJIqjGmIfgCDJ8VAhxDq677jp8Ph/jx49n0aJF7U0zycnJLFmyhJtvvpmcnJz2ks3z5s1j+fLl7Z3FCxcu5MMPPyQnJ4eNGze2n03ffvvtaJrGpEmTWLZsGdnZ2WcV10ApO915uZUrVwKhKzvd2RnLUA9AUoZ6gBmocXUlHOINhxjbhEus4RInDMAy1Eqpu07zsq5p2v+eV0hCCCEGhJ50Fi/l9GfhkgiEEOIC0JNE8AgnEkE8cBfwccgiEkII0afOmAg0TftNx9+VUtuBn4QsIiGEEH2qJ30E73RaPg+whSwiIYQQfaonTUNzO/3eAiwKQSxCCCH6QU8SQVaH536gXNM0KTUhhBAXiDPeUKZp2iEgHaPm0FXAV7sYUhpSSql5SqklBQUFfb1rIYS4oPWkj+AV4LYOL5kwRhEtC1VQpyNTVQohRGj0tI9gC/AmxiQ1QgghLiA9SQTrgQ81TXs81MEIIYToez1JBLHAfyql5gK1wdd0TdNuCF1YQgjRtUBAx+8PYLWaMZlMNDS04PH48Xh8eH0BvF4/Pp8fv98o8xwdHUGju5Wm5koaGloZkhaDx+unqLiaQEAn4A/gD+jGdgMB/H4dPaAzdEgM9ggrR47UYTabSE6Kov54C6Wl9QR0Y/m2yV1OPIcxo5LQgf0HqkhKdBET46C8wk11TSO63mFCsADt6zgcVjLS42ht9bG/sJKRWcew2ywcLqmlqdmLyWTi2jnjuGTaiF7/PnuSCGYEf87s8FrYVaoTQpw/Xdfx+QK0tvrweP14PX7joKnrFBXXEGG3EhMTQVl5A4dLaoMHZz9e74mHx+fHbDIxITuV1lYfn24tYWRmAk6nnQOFlVTXNBMIBNoPrIHAiQOnw2ElNsZJc7OX+uMtxMZEENChoaH1zMGfA7PZKMXt9+uYTSYiIqwEdJ3WVqOV3JgqwNT+3BR80tzsxWw2UVPbhNvdistlo7HREzyggwljYZPJ1L5eq8dE/fEW/H4di9mEp9UHOthsFlyA2WwO2dwEZzt8VAgRJnw+P01NXmJiHAAUFVXT0uqjqdlDU5OXpmYPLc1emlt8tLZ4SU+PI9JlZ9+BShobPWRkxFNX28TefRX4A3r7Ab8zsxkCgc96HJfJBGazidKj9VgtZppbvBQWVeNy2vF4/DgdVqxWMxaLGYvVjNVixmo1Y7VaiHTZSUxw4fPruBtbSEmOxmY109TixW61YI+wYrOdWMdisWA2Q0SEjdraKlJTUzBhwmazYDabOjzMmMwmLMHfTSbjZ3/p60qpPSkxcXYzPQghzlkgYJxtHm/w4KhpJDEhkpYWHzt2HmVYeixmTGz7vJTyCjctLV5aWn14Wn3GWbfPj88bIC7OSWJiJNXVjVRWuUlMjKS11XfGs+a9+ytxOm3ounFwB+MsNCHBhd1uxW63EhFhweGw4Yiw4nBYiYiw4fc1kZaWDJiIsBsHY+PAbcZmtRjPbRZsVnP7QbY/mE2NpKbE9Mu+B7qeXBEIIboRCOi0eny0thoPj8dPa6uPlhYvjY0e3I2tjBubgs8XYMOmIhLiXcTHuzhwoIqDxdV4vYFge/bJZ9xmswmLxXzK1JBtjDPrtgOumQi7legoC9FREURHRRATE8GIEfEkJ0UREWHFZDIRGWnD6bATEWE98bBbsNut53wGbJy9dj1HsBj4JBGIC1pbm7anQxt1ZVUTkVHNxMU6qalp5FjZcbKyEmlt9fPJp4dobfHR6vGd6Hz0+PF4jeejRyURFRnB7j3ltHp8ZKTHUVPdRNGh7iftW/n+3vbnFosJp8OGOTgvcKTLRkSEk4gIK06HDYfTRiDgIS01gfh4l3H2HWElwmEcuB3BA7jVagnpdycGD0kEos/putHe7PX6TzpAD0kzLtv37qsgJSWKuFgnBwqrKD5cQ2uL76RmEI/Xh8cbINJlY8L4NGrrmtn86SEmXzQEs9nM5zuP0dDQQlcT8Nls+zCZTHg8pz/b7sxiNmG2mNi5qwyny4bfF8Bqs9Da4iMm1sFFE9JwOm04XTYiXXYiIyOIirTjdNiIcFhxRBg/I+xGs8mZhNNsWiL8SSIQZ9R2Vm2zWfD7AxwrO240g3j8tLZ4OXaskl27G2hs8tDS4mPC+FQcDhufaocxW0yMzEykpLSO7Z8fxe83huadTlpqNF6vn5raJqwWc/twvu5YLCZKSuqMs2Mdig/V4nTaiImOIC7Wid1mwWa3YLdbgs0gVjytTaSmJRLpsmO1mo327+ByNmtweZulvV3bYunJ1N5ChC9JBINMWxu0yWSi5Egd9fXNuN2tuBs9NDS0UF/fwvGGFtxuDyOzEklLi2bP3gqOHqtn3NgU3O5WCg9Wd7uPz7afmGTbajVTeLAai9mEw2E1DrQ242G3W4iwG80c9ggrMdERwSYPs/GazXLSQfrEQd14rW0M+dmSs20hTiaJ4ALR3OKlvq6Z+uMt7Qd2t7sVt7uV6OgIVN5wioqqWf7ODq6+cgweb4BPNh+iucXb3knY+ex71xfHOHykFrvNTHJyFABJiZGkJEcRFRVBZGQEkS47jU31ZKQPMQ7ewbNvu82CxRK6cc9CiN4jiSCM+P2B9ucfbzjIyMxEhg6N5d1/7OKj9Qfb37NYjGF6Rlt8AF2HdQWFgDESZfOWw8REO8kcEU9srIPoaCfR0RFER0cQE+0gOiqCqCh7jzsjS0t1hg2L69XPKoToO5IIBiBd16mocHP0WD1l5Q2UVzRQXt5ATW0T//bdOKprGtmwsZgdO4/h9wcoK29oXzcy0k5qSjQpyVEkJUUaB/boCKKDPx3BYYRCCNEmbBKBUmoeMO/JJ59k9uzZ/R1Or/H7AzS4W4mLdVJ/vIW339lByZE6jh9vwWYzExfrxGI17nq02cw8+fQ6TCZITIjE4bCRmhLFJRePICU5iuTkKFxOe39/JCFEmAmbRHAhzEcQCOhUVLo5WFRNUqKLsWNSKPj4IJ9sPsSc/LEcLKqmrOw48XFOIiIs1NQ0UVnVSGpKFFmZiUS6dC6aOIK01Gjs9rD50wkhBjg5moRQc7OX0qP1HC6ppfhQDYcO19Lc7MXltDFp0lB2f1HOnn3l1NQ289bfdzA8I47ExEi8Xj8Txw9h6JAYMtKN16BttEt8P38qIcSFRhJBLwkEdI6U1hEf5yQ62sHyv3/Oxk+MMk0J8S4SE10Mz4invr6FsvLjfKodZnhGHLlTMhgzOomM9Pge3WgkhBC9TRLBOfL5/Bwra6DkSB3TLzbqg7+8bDNfunY8F6vhZI9LxWI1U1bWwKHDtdTUNpGaEs2Y0Ulcd202IzMTcDhs/fwphBBCEkGP1dU1c7C4mkOHajlSWsfRY8fx+wNERdrJHpdCQryLhfdNp6iohhf+sIGi4mpsNgvjx6Vy842TGD0qmdhgOWAhhBhIJBGchq7r1B9vIS7WSWOTh2d+W0BtXTM2m5mM9HhGZiVy+axRZKTH4fMH+HzHUXbsPEbJkTpcThsTJqRx+axRjB6VhM0mhcGEEAObJAKM9v26+mZ8vgApyVFs217Km29/zs9/ch0up41ZM0eSkRHHsKFxWCwmysoa2LHrGKvX7qOsrIHo6AgmTRzCl64dT1ZmgtSmEUKElUGXCFpavBwpraes7DiFB8s43nCQ8ooGPB4/o0cmcf83ZjBubArfvn9mcBo5E5fNHInb3cqHHx1A23qE6upGEuJdXHTREG65cTIZ6fH9OpuREEKcj0GVCDZtPsQ77+3E5wsQG+MgLs7OyKxkLp2RSVpqDAkJLgBcLjsul3Fj1pHSOtZvKGLb50dxOqxMU8PJmTSUIUNi5A5dIcQFYVAlguEZcdx6Uw7Z41JwuezdVqGsqW3i7Xd2sGdvBRnpcdx682RyJg2VyUCEEBecQZUIhg6JZeiQ2G6Xqaxys2btfrZ9fpSUlCgeuP9SsjIT+yhCIYToe4MqEXTH6/WzavVePlp/kNTUaBbcnEPO5KHS8SuEuOBJIsC4Cli6bDONjR5uuWkyeVMzpPNXCDFoDPpEcKS0jhdf/oSUlCi+tXAm0dER/R2SEEL0qUGdCNxuD397axcZGXHc+TUlN38JIQalQdsAHgjo/GNFEVGREdx+W54kASHEoDVoE8Gu3ccor2jk9ttyiYgY1BdGQohBblAmgkBAZ9XqfWSPTSAlJbq/wxFCiH41KBPBnn0VVFQ2cPG0If0dihBC9LtBmQh27DzKyKxE4mJlhJAQQgy6ROD3B/jii3IumihXA0IIAYMwERwsqqap2cvECWn9HYoQQgwIAyIRKKWuU0q90xf72rW7jIz0OOJinX2xOyGEGPBCNm5SKRULvA9MAKZrmrYz+PrjwKVAMXAfMAaIAQ6GKpaOjh6rJyszoS92JYQQYSGUVwRNwPXAG20vKKVygGGaps0C9gC3AtcCQ4GpwfdDqrqmicSEyFDvRgghwkbIrgg0TfMClUqpji9fCqwKPl8B3Ktp2oMASqlMTdO2n25bSqn7gfsBHnzwQebMmXNOMXm9fhoaWtH1ZkpLS/F6vZSWlp7TtvrDQI13oMbVlXCINxxibBMusYZLnBCaWLuaewX6vtZQPHAs+LweaG+j0TTt+12tpGnaEmBJ8Ff9XHd+rOw4AOPGDicxMbLbiWkGooEa70CNqyvhEG84xNgmXGINlzih72Pt687iOoz+AIBYoKYvd15d3YjZbCIuTjqKhRCiTV8ngg3A1cHn1wLr+3Ln1TVNxMU5ZbIZIYToIKRHRKXUP4FrgD8ope7RNG0bUK6U+giYCLx5Ftuap5RaUlBQcM7xVFc3khicoF4IIYQhpH0Emqb9y2lee/gct/Uu8C6w8Fzjqa5plBFDQgjRyaBqI6muaSIxURKBEEJ0NGgSgd8foK6uWZqGhBCik0GTCFo9PsaPSyE1VeYfEEKIjsImEZxvZ7HLaefuOy8mOSmqlyMTQojwFjZzNPZGZ7EQQohThc0VgRBCiNCQRCCEEIOcJAIhhBjkwiYR9MadxUIIIU4lncVCCDHIhc0VgRBCiNCQRCCEEIOcSdfPeZ4XIYQQFwC5IhBCiEFOEoEQQgxykgiEEGKQk0QghBCDnCQCIYQY5CQRCCHEICeJQAghBrmwKTExECilMgEbcEDTtAF/A4ZSKgMYB2zSNM3d3/G0UUqlAznAZ5qmHe3veHpCKTUX2K1p2sH+juV0gn/rycAxYJumaYF+DqlLSqk0IBeIBN7VNK2ln0M6rQ7/f6YASzRNO96/EXUt+H8qF/hC07T9Z7u+JILTUErZganACGCFpmnHlVKPAA8D7wA/Asr6McSTdIr3/zRNa1BKvQCMBRqBKUqp1zRNO9KPcbV9j/8FKMANHFVK/V7TtB19GVdXOsX7T03T3EopE+AAXgb+C3iqH0Ps6jt9HRgGbALexrjS7/dE0MX3aQN+gHFCtR6w9GOIQJf/f34PjAGqMP4ffQB81n9RGrqI9Trge8BxoFkp9VtN07Sz2a40DXUQ/E8PcBvwCDAP+JpSKgp4ErgO2AakdVq+X3QTbzzwY03TrtI0bR4QA8zqtE5/xBUF/EjTtGs0TbsZOApc3FdxdaWbeJOCV37zgT1AvVLKMYBivDP4WiHwN+DnwC5N03wD8Ptsi/Ve4AjwNLBV07TGvo/Q0EWctyulYjRN+6amaVcBvwD+DBzupzCBM36n+RhXVl/B+G6/1GmdMxo0iSB4EEIpFamU+olSqkYpdXfwNQuApml68HkW8FvgLiAemKdpmg+ow7icTR7A8SYA12qaVhVcNgOIAra0rdNPccUD1wffcyqlbg3GWtAbcYUg3kTg6uBmfMBWIBNwDqAYE5RSVwJrgG8DLwGPKaUuCy7f68ngPGJNUkpdA+zDSAYPAw8ppR4Krturx6Lz/Hf6pQ6bmgBM1TSturdj7IVYE4N//1XASKXUtRjHp1Vt6/Q0hgsuESilEpVSycHnlyil/qqU+hz4jVIqL3gG8g7wHMYlVtt6bZeoaUAL0BT8IncBecH3aoEmIBV65+AV4ngJPo/UNG3fAIjr4uB7Q4AHg8vcGVzvvA5aIYj3C2CiUioWmA78BEjC+PsPlBg/x0j6q4DxmqbdgpEUvn+uMYYw1h0YV6UBIFHTtAeA/wFuDC5/Tv+XQvTvVHXYRQ3gOZ8YQxjrDmCOpmnvB2NbDMQB1wbXu/CvCJRSFqVUjlLqJqXUlOBrP8c4w1yklMoCDgB/0DRtMkbb6X8BaJq2HfgrRhsgQEDTNH/weSUQgXEWDcalVmRwvZrg+wM93ujg9l0Y/9F+MEDicgXXO6hp2hWapt0B3KWUMvU0qfZhvIcxrgriMJrWRgLjgaeVUhEDJMYSIDa4Xtsy64Honn6nfRxrPLAdo92d4MlJnFLKfKZY+/jfadsZuhmjA/bd4HYG2r/REiBWKRUHfEnTtKmapt0LfFMpZTmbE9Ww7CwOZrp7gTuAIqBcGSN6sjVNm6iUuhr4paZpX1NKfRBc7QhQ3WEzxRhneG2XXVcDX8Y4IyjHuCT8AKMZ6KBSyhEc3ZABXKqMJpfnNU2rH4jxBtf5N4wzsBuUUjHAq5qm1fXz9xiB0VE4AWOUy58wOmObB9j3mILxn/UijCvA7wU/ww66OYHqh++0MJjw7RgdiF8BXgNMnOEMto9jTQJKNE2rV0odVErdjJFg12GcaDUMkDhP+v+ujI7Yh7r7Hvsx1iTgEMbAi2PKGNkWi/GdxmJczfRIWF4RBDPde8GzynsxmmxyMEbIgHF5PLLDsgDfwWhDJXi25AZ8Sqmk4Ptu4C3gfzEuz6Yqo53uLuDT4D+K2zEuG0swOuh8AzTezcFlvoQxmmQ8UMGJS9z+ikvTNK0VowPuJxjD8v6iadoZk0A/xHsnsFnTtH9omnZjcH/fAX7VXbz99G+zCfhv4NcYCWGV1oPho/0Q6yfBZb6N0fRxBcZZcZdJoL/i1E4Mad0CeLuLr59j/VQz+i+fxLj6zweeDe63x8LyigBA07Sy4CVlALgEY9REtFIqQdO0GqVUs1JqlKZphcroUPFqmrY6uLoZ8GNc/l+tlFqladqmjttXSi0F5mJcam8L7vMV4JUwiveyARbXZ8F99vgMq5/j3dnhdYumaR8OwBi3B/f59Z7E1s+xtv39yzBOBAZqnNs77Pfhs4mzH2Jt+07/D/i/s421TdgmAgBN0wJKqSswhiFux+gYHYlxSVQMjMI4c78NWKGUGgnYNE3bq5T6FsaokClApFLqz5qmtQYzsq5pWgHBES0XerwDNa6BFK92op12wMZ4rsIl1nCJM9xihTBOBOpEZ9gNwO80TftCKRXAaL9vwvgDaEqp2zDa60ZgtJv+FtgL/B14pfNlaYfLtUER70CNK5zjDYcYwy3WcIkz3GJtE9ZTVSqlHsD4Il/DSGp7MdoebwJ+r2naH5RSiYBD07TS/ovUMFDjHahxdSUc4g2HGNuES6zhEieEV6wQxolAGWNrf4LRefsxxpjqAm0A1dTpaKDGO1Dj6ko4xBsOMbYJl1jDJU4Ir1jbhG0iEEII0TvCcvioEEKI3iOJQAghBjlJBEIIMchJIhBCiEFOEoEQQgxyYXtDmRD9QSn1A+AJ4F5N05Z2sYwLY/KQ4q6WEWIgkSsCIXqfC/gpcE8/xyFEj8h9BEKcQfAqYBFGBddPMao+3gtcj1ETxolR+vtHmqYtV0oVY5QNaPNzjHrz/wV8FaPs8vvAtzVNO6f5LYToTZIIhOiGUioHo5rrLuAZjDP9oRiJIAWj3G8UsBBjropk4GaMKrVfYJTc3gncAvwM+D1QhjFZ0ErNmFVMiH4lfQRCdO+K4M+nNE17URkTEv0YsAATMapH2jssn0lwzligQtO01wCUUi8HX/tmh2XnhChmIc6KJAIhesbU6acNo4loNfAb4F8xmoocdD0zmA+jjnxbSWvpoxMDgiQCIbq3Lvjz+8qYw/beTu9HYswvO7PDa8cxpggdrYxZ7T4G3sMoQnY3RvKYAGRx4upBiH4jZyRCdEMzJhN/GEjDOOtvm6XMi1FieApG89DKDut4MYaYxgF/BmYBvwq+Nguj7vyXOmxLiH4lncVCCDHIyRWBEEIMcpIIhBBikJNEIIQQg5wkAiGEGOQkEQghxCAniUAIIQY5SQRCCDHI/X+IDl7M/MQaMwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# copy from Sidarthe model\n",
    "input_steps=1\n",
    "output_steps=2\n",
    "\n",
    "start_index = VAL_IDX+input_steps+output_steps\n",
    "x = last_update_pydatetime[start_index:]\n",
    "\n",
    "plt.plot(x, predictions[input_steps+output_steps:,0], label ='predict infected', color=confirmed_color )\n",
    "plt.plot(x, canada[start_index:][:,0], '-.', label ='actual infected', color=confirmed_color )\n",
    "plt.plot(x, predictions[input_steps+output_steps:,1], label ='predict recovered', color=healed_color)\n",
    "plt.plot(x, canada[start_index:][:,1], '-.', label ='actual recovered', color=healed_color)\n",
    "plt.plot(x, predictions[input_steps+output_steps:,2], label ='predict death', color=death_color)\n",
    "plt.plot(x, canada[start_index:][:,2], '-.', label ='actual death', color=death_color)\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"date\")\n",
    "plt.ylabel(\"number of infected\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=10)\n",
    "tikzplotlib.save(\"arima.tikz\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "4c2c2b6f-f582-4652-aae6-14ccfc5de988",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-02T00:56:46.675880Z",
     "iopub.status.busy": "2022-10-02T00:56:46.675646Z",
     "iopub.status.idle": "2022-10-02T00:56:46.682909Z",
     "shell.execute_reply": "2022-10-02T00:56:46.681763Z",
     "shell.execute_reply.started": "2022-10-02T00:56:46.675856Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32189.883983899264"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(predictions[3:,0], canada[VAL_IDX:][3:,0], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "411d99e3-b2bf-4fd3-bcf9-ba63dcbf522d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-01T23:51:27.854962Z",
     "iopub.status.busy": "2022-10-01T23:51:27.854637Z",
     "iopub.status.idle": "2022-10-01T23:51:27.862512Z",
     "shell.execute_reply": "2022-10-01T23:51:27.860754Z",
     "shell.execute_reply.started": "2022-10-01T23:51:27.854924Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28208.251824218773"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(predictions[3:,1], canada[VAL_IDX:][3:,1], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cc324267-d802-488c-8cd3-0a39204df8cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-10-01T23:51:35.999814Z",
     "iopub.status.busy": "2022-10-01T23:51:35.999513Z",
     "iopub.status.idle": "2022-10-01T23:51:36.006960Z",
     "shell.execute_reply": "2022-10-01T23:51:36.005564Z",
     "shell.execute_reply.started": "2022-10-01T23:51:35.999780Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "809.9104266812906"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(predictions[3:,2], canada[VAL_IDX:][3:,2], squared=False)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# coding: utf-8",
   "executable": "/usr/bin/env python",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
